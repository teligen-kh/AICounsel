문제점들:
DB 검색 모듈이 제대로 작동하지 않음
"DB 늘리기 방법" 질문에 대해 DB에서 관련 답변을 찾지 못함
키워드 매칭 로직에 문제가 있음
LLM 모듈이 불필요하게 복잡함
DB 답변이 있는데도 LLM으로 개선하려고 시도
처리 시간이 6분이나 걸림
모듈 간 의존성이 복잡함
하나가 고장나면 전체가 작동 안 함


해결 방안:
DB 검색 모듈을 독립적으로 만들기:
키워드 추출 → AND 조건 검색 → 가장 적절한 답변 선택
점수 기반 랭킹 시스템
LLM 모듈을 선택적으로 사용:
DB에서 답변을 찾으면 그대로 사용
찾지 못했을 때만 LLM 사용
어떤 부분부터 수정하시겠습니까?
DB 검색 모듈을 먼저 고치기
LLM 모듈을 단순화하기
모듈 간 의존성 분리하기


전체 프로젝트 모듈화  및 내가 원할 때 빼고 수정 금지 작업  진행.
polyglot-ko-5.8b 모델 한국어 테스트
언어모델  + DB 연결 테스트


내일 진행할 작업들:
1. 모듈화 개선
순환 import 문제 완전 해결
의존성 주입 구조 정리
서비스 간 결합도 낮추기
2. DB 연결 및 검색 개선
MongoDB 연결 상태 확인
키워드 매칭 알고리즘 개선
관련성 높은 답변 검색 로직 수정
3. LLM 통합
Polyglot-Ko 모델 프롬프트 최적화
안정적인 응답 생성 보장
속도와 품질의 균형 맞추기
4. 전체 시스템 안정화
에러 처리 강화
로깅 시스템 개선
성능 모니터링 추가

2025-07-04
서버 테스트 및 최종 검증
프론트엔드 연동 확인
성능 모니터링 및 최적화
추가 기능 개발


주요 문제점 분석
1. LLM 처리 시간이 비정상적으로 길음
일상 대화: 331초 (5분 31초) - 단순한 "안녕하세요?"에 대해
전문 상담: 27초~62초 - 이마저도 너무 김
정상적인 LLM 응답은 보통 1-5초 내외여야 함
2. 응답 품질 문제
일상 대화 응답: "네, 안녕하세요?" --토트(dmthoth) 2012년 10월 17일 (목) 14:24 (KST)
위키피디아 스타일의 인용문이 나옴
실제 대화에 적합하지 않은 형식
전문 상담 응답: )렬쇄답변: - 깨진 텍스트가 나옴
3. 검색 로직 문제
키워드 매칭이 항상 같은 항목(02-2026-8945 포스 프로그램 재설치)을 찾음
검색 점수가 20-21점으로 낮은데도 매칭됨
검색 결과가 실제 질문과 관련성이 떨어짐
4. 모델 로딩 중복
서버 시작 시 모델 로딩
요청 시마다 다시 모델 로딩 체크
메모리 효율성 문제
해결 방향 논의
어떤 부분부터 우선적으로 해결하고 싶으신가요?
LLM 처리 시간 최적화 - 가장 시급한 문제
응답 품질 개선 - 프롬프트 엔지니어링
검색 로직 개선 - 더 정확한 매칭
모델 관리 최적화 - 메모리 효율성


LLM 모델을 다시 LLaMA 3.1 로 돌아가야할 상황임
현재 Polyglot-Ko 최적화 작업 중



2025-07-10
서버 컴퓨터에서 파인튜닝을 진행하기 위한 단계별 작업을 설명드리겠습니다.
1. 서버 컴퓨터 준비 작업
1.1 필요한 소프트웨어 설치
Apply to model_manage...
Run
1.2 프로젝트 파일 복사
개발 컴퓨터에서 서버 컴퓨터로 다음 파일들을 복사해야 합니다:
backend/finetune_tool/ 폴더 전체
backend/test_tool/ 폴더 전체
상담 데이터 CSV 파일들
FAQ 데이터 CSV 파일들
2. 파인튜닝 도구 설치 및 설정
2.1 의존성 설치
Apply to model_manage...
Run
2.2 설정 파일 확인
config.py 파일에서 다음 설정들을 확인/수정:
3. GUI 파인튜닝 도구 실행
3.1 GUI 실행
Apply to model_manage...
Run
3.2 GUI에서 설정할 항목들:
모델 선택: Phi-3.5-mini 또는 다른 모델
데이터 경로: 상담 데이터와 FAQ 데이터 파일 경로
학습 파라미터:
Epochs: 3-5 (처음에는 적게)
Learning Rate: 2e-4 (기본값)
Batch Size: GPU 메모리에 따라 조정
LoRA 설정: 기본값 사용
출력 경로: 모델 저장 위치
4. 파인튜닝 실행 과정
4.1 데이터 전처리
GUI에서 "데이터 전처리" 버튼을 클릭하여:
CSV 데이터를 학습용 형식으로 변환
훈련/검증/테스트 데이터 분할
데이터 품질 검사
4.2 모델 다운로드
"모델 다운로드" 버튼을 클릭하여:
선택한 모델을 로컬에 다운로드
모델 크기 확인 (Phi-3.5-mini는 약 3.8GB)
4.3 파인튜닝 시작
"파인튜닝 시작" 버튼을 클릭하여:
LoRA 파인튜닝 시작
진행 상황 모니터링
로그 확인
5. 파인튜닝 완료 후 작업
5.1 모델 테스트
Apply to model_manage...
Run
5.2 모델 압축 및 변환
Apply to model_manage...
Run
5.3 개발 컴퓨터로 전송
파인튜닝된 모델 파일들을 개발 컴퓨터로 복사:
finetuned_models/ 폴더 전체
설정 파일들
테스트 결과
6. 예상 소요 시간
GTX 1050 Ti 기준:
데이터 전처리: 5-10분
모델 다운로드: 10-30분 (인터넷 속도에 따라)
파인튜닝: 2-4시간 (3 epochs 기준)
테스트: 10-20분
7. 문제 해결
7.1 메모리 부족 시
Batch size를 1로 줄이기
Gradient accumulation steps 증가
4bit 양자화 사용
7.2 CUDA 오류 시
CUDA 버전 확인
GPU 드라이버 업데이트
메모리 사용량 모니터링
7.3 학습 중단 시
체크포인트에서 재시작 가능
GUI에서 "재시작" 버튼 사용
이렇게 하면 서버 컴퓨터에서 파인튜닝을 완료하고, 결과 모델을 개발 컴퓨터에서 테스트할 수 있습니다. 특별히 궁금한 부분이나 추가 설명이 필요한 부분이 있으시면 말씀해 주세요!



2025-07-11
다음에 할 일 📋
백엔드 오류 수정 (Permission.COUNSELING_VIEW → COUNSELING_ACCESS)
프론트엔드-백엔드 연동 테스트
파인튜닝 시스템 구축
실제 아름넷 프로그램 연동