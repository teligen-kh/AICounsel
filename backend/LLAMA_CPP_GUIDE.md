# llama-cpp-python í†µí•© ê°€ì´ë“œ

## ğŸ“‹ ê°œìš”

ì´ ê°€ì´ë“œëŠ” AI ìƒë‹´ ì„œë¹„ìŠ¤ì— llama-cpp-pythonì„ í†µí•©í•˜ì—¬ CPU í™˜ê²½ì—ì„œ íš¨ìœ¨ì ìœ¼ë¡œ LLMì„ ì‹¤í–‰í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.

## ğŸ¯ ëª©í‘œ

- **ì„±ëŠ¥ í–¥ìƒ**: CPUì—ì„œ ë” ë¹ ë¥¸ ì¶”ë¡  ì†ë„
- **ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±**: ë” ì ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
- **í™•ì¥ì„±**: GPU ì§€ì› ì¤€ë¹„
- **ì•ˆì •ì„±**: ê¸°ì¡´ ì•„í‚¤í…ì²˜ì™€ì˜ í˜¸í™˜ì„± ìœ ì§€

## ğŸš€ ì„¤ì¹˜ ë° ì„¤ì •

### 1. ìë™ ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰

```bash
cd backend
python install_llama_cpp.py
```

### 2. ìˆ˜ë™ ì„¤ì¹˜ (ì„ íƒì‚¬í•­)

#### íŒ¨í‚¤ì§€ ì„¤ì¹˜
```bash
pip install llama-cpp-python
```

#### GGUF ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
```bash
# Polyglot-Ko 5.8B
wget -O models/polyglot-ko-5.8b-Q4_K_M.gguf \
  https://huggingface.co/TheBloke/polyglot-ko-5.8B-GGUF/resolve/main/polyglot-ko-5.8b.Q4_K_M.gguf

# LLaMA 3.1 8B Instruct
wget -O models/llama-3.1-8b-instruct-Q4_K_M.gguf \
  https://huggingface.co/TheBloke/Llama-3.1-8B-Instruct-GGUF/resolve/main/llama-3.1-8b-instruct.Q4_K_M.gguf
```

### 3. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

```bash
# Windows
set USE_LLAMA_CPP=true

# Linux/Mac
export USE_LLAMA_CPP=true
```

ë˜ëŠ” `.env` íŒŒì¼ ìƒì„±:
```env
USE_LLAMA_CPP=true
DEFAULT_MODEL_TYPE=polyglot-ko-5.8b
LLAMA_CPP_N_THREADS=8
LLAMA_CPP_N_CTX=2048
```

## ğŸ”§ ì‚¬ìš© ë°©ë²•

### 1. ì„œë²„ ì‹¤í–‰

```bash
cd backend
python -m uvicorn app.main:app --reload
```

### 2. API í…ŒìŠ¤íŠ¸

```bash
# ê¸°ë³¸ ì±„íŒ… í…ŒìŠ¤íŠ¸
curl -X POST "http://localhost:8000/api/v1/chat/send" \
  -H "Content-Type: application/json" \
  -d '{"message": "ì•ˆë…•í•˜ì„¸ìš”"}'

# ìƒì„¸ ì‘ë‹µ í™•ì¸
curl -X POST "http://localhost:8000/api/v1/chat/send" \
  -H "Content-Type: application/json" \
  -d '{"message": "ê³ ê° ìƒë‹´ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”"}' \
  -v
```

### 3. ëª¨ë¸ ì „í™˜

```python
# ì½”ë“œì—ì„œ ëª¨ë¸ ì „í™˜
llm_service = await get_llm_service()
llm_service.switch_model("polyglot-ko-5.8b")  # ë˜ëŠ” "llama-3.1-8b"
```

## ğŸ“Š ì„±ëŠ¥ ë¹„êµ

### Transformers vs llama-cpp-python

| í•­ëª© | Transformers | llama-cpp-python |
|------|-------------|------------------|
| ì´ˆê¸° ë¡œë”© | 30-60ì´ˆ | 5-15ì´ˆ |
| ì‘ë‹µ ì‹œê°„ | 15-25ì´ˆ | 3-8ì´ˆ |
| ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ | 8-12GB | 2-4GB |
| CPU ì‚¬ìš©ë¥  | ë†’ìŒ | ì¤‘ê°„ |
| GPU ì§€ì› | ì™„ì „ | ì œí•œì  |

### ìµœì í™”ëœ íŒŒë¼ë¯¸í„°

#### Polyglot-Ko 5.8B
```python
{
    "max_tokens": 50,        # ì§§ì€ ì‘ë‹µìœ¼ë¡œ ì†ë„ í–¥ìƒ
    "temperature": 0.5,      # ì¼ê´€ì„± ìˆëŠ” ì‘ë‹µ
    "top_p": 0.7,           # ì•ˆì •ì ì¸ ìƒì„±
    "top_k": 40,            # í’ˆì§ˆê³¼ ì†ë„ ê· í˜•
    "repeat_penalty": 1.1,  # ë°˜ë³µ ë°©ì§€
    "stop": ["\n\n", "ì‚¬ìš©ì:", "ì§ˆë¬¸:"]  # ìì—°ìŠ¤ëŸ¬ìš´ ì¤‘ë‹¨
}
```

## ğŸ› ï¸ ë¬¸ì œ í•´ê²°

### 1. ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨

**ì¦ìƒ**: `FileNotFoundError: Model file not found`

**í•´ê²°ì±…**:
```bash
# ëª¨ë¸ íŒŒì¼ í™•ì¸
ls -la models/*.gguf

# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¬ì‹œë„
python install_llama_cpp.py
```

### 2. ë©”ëª¨ë¦¬ ë¶€ì¡±

**ì¦ìƒ**: `OutOfMemoryError`

**í•´ê²°ì±…**:
```python
# llama_cpp_processor.pyì—ì„œ íŒŒë¼ë¯¸í„° ì¡°ì •
self.llm = Llama(
    model_path=self.model_path,
    n_ctx=1024,        # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì¤„ì´ê¸°
    n_threads=4,       # ìŠ¤ë ˆë“œ ìˆ˜ ì¤„ì´ê¸°
    n_gpu_layers=0,    # GPU ì‚¬ìš© ì•ˆí•¨
    verbose=False
)
```

### 3. ì‘ë‹µ í’ˆì§ˆ ì €í•˜

**ì¦ìƒ**: ë¬´ì˜ë¯¸í•œ ì‘ë‹µ ë˜ëŠ” ë…¸ì´ì¦ˆ

**í•´ê²°ì±…**:
```python
# í›„ì²˜ë¦¬ í•„í„°ë§ ê°•í™”
meaningless_patterns = [
    r'â€».*ë°œìƒí•˜ê³  ìˆìŠµë‹ˆë‹¤',
    r'ì•ˆë…•í•˜ì„¸ìš©~',
    r'ã…ã…',
    # ì¶”ê°€ íŒ¨í„´...
]
```

### 4. ì„±ëŠ¥ ìµœì í™”

**CPU ìŠ¤ë ˆë“œ ìˆ˜ ì¡°ì •**:
```python
# ì‹œìŠ¤í…œì— ë§ê²Œ ì¡°ì •
n_threads = min(8, os.cpu_count())  # CPU ì½”ì–´ ìˆ˜ì— ë§ì¶¤
```

**ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ìµœì í™”**:
```python
# ëŒ€í™” ê¸¸ì´ì— ë§ê²Œ ì¡°ì •
n_ctx = 2048  # ì§§ì€ ëŒ€í™”: 1024, ê¸´ ëŒ€í™”: 4096
```

## ğŸ”„ í´ë°± ë©”ì»¤ë‹ˆì¦˜

llama-cpp-python ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ ìë™ìœ¼ë¡œ Transformersë¡œ í´ë°±ë©ë‹ˆë‹¤:

```python
try:
    self._initialize_llama_cpp()
except Exception as e:
    logging.warning("llama-cpp ì´ˆê¸°í™” ì‹¤íŒ¨, Transformersë¡œ í´ë°±")
    self.use_llama_cpp = False
    self._initialize_transformers()
```

## ğŸ“ˆ ëª¨ë‹ˆí„°ë§

### ì„±ëŠ¥ ë©”íŠ¸ë¦­ í™•ì¸

```python
# ì‘ë‹µ í†µê³„ í™•ì¸
stats = llm_service.get_response_stats()
print(f"í‰ê·  ì²˜ë¦¬ ì‹œê°„: {stats['total_processing_time'] / stats['total_requests']:.2f}ms")
print(f"ì„±ê³µë¥ : {(stats['total_requests'] - stats['errors']) / stats['total_requests'] * 100:.1f}%")
```

### ë¡œê·¸ í™•ì¸

```bash
# ì‹¤ì‹œê°„ ë¡œê·¸ ëª¨ë‹ˆí„°ë§
tail -f backend/logs/app.log | grep "llama-cpp"
```

## ğŸš€ GPU ì§€ì› (í–¥í›„ ê³„íš)

### CUDA ì§€ì› ì„¤ì¹˜

```bash
# CUDA ì§€ì› ë²„ì „ ì„¤ì¹˜
pip install llama-cpp-python[server] --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/
```

### GPU ì„¤ì •

```python
self.llm = Llama(
    model_path=self.model_path,
    n_gpu_layers=35,  # GPU ë ˆì´ì–´ ìˆ˜ (ëª¨ë¸ í¬ê¸°ì— ë”°ë¼ ì¡°ì •)
    n_threads=8,
    verbose=False
)
```

## ğŸ“ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] llama-cpp-python ì„¤ì¹˜ ì™„ë£Œ
- [ ] GGUF ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ
- [ ] í™˜ê²½ ë³€ìˆ˜ ì„¤ì • ì™„ë£Œ
- [ ] ì„œë²„ ì‹¤í–‰ ë° í…ŒìŠ¤íŠ¸ ì™„ë£Œ
- [ ] ì„±ëŠ¥ í™•ì¸ ì™„ë£Œ
- [ ] í´ë°± ë©”ì»¤ë‹ˆì¦˜ í…ŒìŠ¤íŠ¸ ì™„ë£Œ

## ğŸ†˜ ì§€ì›

ë¬¸ì œê°€ ë°œìƒí•˜ë©´ ë‹¤ìŒì„ í™•ì¸í•˜ì„¸ìš”:

1. **ë¡œê·¸ í™•ì¸**: `backend/logs/app.log`
2. **ëª¨ë¸ íŒŒì¼ í™•ì¸**: `models/` ë””ë ‰í† ë¦¬
3. **í™˜ê²½ ë³€ìˆ˜ í™•ì¸**: `echo $USE_LLAMA_CPP`
4. **ì˜ì¡´ì„± í™•ì¸**: `pip list | grep llama-cpp`

ì¶”ê°€ ì§€ì›ì´ í•„ìš”í•˜ë©´ í”„ë¡œì íŠ¸ ì´ìŠˆë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”. 