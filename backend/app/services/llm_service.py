from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
from langchain_mongodb import MongoDBChatMessageHistory
from ..config import settings
from .mongodb_search_service import MongoDBSearchService
from .conversation_algorithm import ConversationAlgorithm
from .formatting_service import FormattingService
from .model_manager import get_model_manager, ModelType
from .llm_processors import LLMProcessorFactory, BaseLLMProcessor
from .llama_cpp_processor import LlamaCppProcessor
from .finetuned_processor import get_finetuned_processor
from motor.motor_asyncio import AsyncIOMotorDatabase
import os
import logging
import re
import time
from datetime import datetime
from typing import Dict, List, Tuple, Optional
from enum import Enum
import asyncio

class IntentType(Enum):
    """ì‚¬ìš©ì ì˜ë„ íƒ€ì…"""
    CASUAL = "casual"  # ì¼ìƒ ëŒ€í™”
    PROFESSIONAL = "professional"  # ì „ë¬¸ ìƒë‹´
    GREETING = "greeting"  # ì¸ì‚¬
    QUESTION = "question"  # ì§ˆë¬¸
    COMPLAINT = "complaint"  # ë¶ˆë§Œ/ë¬¸ì œ
    REQUEST = "request"  # ìš”ì²­
    THANKS = "thanks"  # ê°ì‚¬
    UNKNOWN = "unknown"  # ì•Œ ìˆ˜ ì—†ìŒ

class LLMService:
    def __init__(self, use_db_mode: bool = False, use_llama_cpp: bool = False, use_finetuned: bool = False):
        """
        ìˆœìˆ˜ LLM ì„œë¹„ìŠ¤ ì´ˆê¸°í™” (DB ì˜ì¡´ì„± ì œê±°)
        
        Args:
            use_db_mode: DB ì—°ë™ ëª¨ë“œ ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: False)
            use_llama_cpp: llama-cpp-python ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: False)
            use_finetuned: íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: False)
        """
        self.use_db_mode = use_db_mode
        self.use_llama_cpp = use_llama_cpp
        self.use_finetuned = use_finetuned
        self.model_type = "llama-3.1-8b-instruct"  # ê¸°ë³¸ ëª¨ë¸
        
        # ì›ë³¸ Llama-3.1-8B-Instruct ëª¨ë¸ ì§ì ‘ ì´ˆê¸°í™”
        self._initialize_original_llama()
        
        # DB ì„œë¹„ìŠ¤ëŠ” ì™¸ë¶€ì—ì„œ ì£¼ì…ë°›ìŒ (ì˜ì¡´ì„± ë¶„ë¦¬)
        self.search_service = None
        
        # ì‘ë‹µ ì‹œê°„ í†µê³„ ì´ˆê¸°í™”
        self.response_stats = {
            'total_requests': 0,
            'llama_responses': 0,
            'db_responses': 0,
            'errors': 0,
            'total_processing_time': 0,
            'llama_processing_time': 0,
            'db_processing_time': 0,
            'error_processing_time': 0,
            'min_processing_time': float('inf'),
            'max_processing_time': 0,
            'processing_times': []
        }

    def _initialize_original_llama(self):
        """ì›ë³¸ Llama-3.1-8B-Instruct ëª¨ë¸ ì§ì ‘ ì´ˆê¸°í™”"""
        try:
            from transformers import AutoTokenizer, AutoModelForCausalLM
            import torch
            
            # ëª¨ë¸ ê²½ë¡œ ì„¤ì •
            model_path = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(__file__)))), 
                                    "models", "Llama-3.1-8B-Instruct")
            
            logging.info(f"ì›ë³¸ Llama-3.1-8B-Instruct ëª¨ë¸ ë¡œë”© ì¤‘: {model_path}")
            
            # í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ ë¡œë“œ
            self.tokenizer = AutoTokenizer.from_pretrained(model_path)
            self.model = AutoModelForCausalLM.from_pretrained(
                model_path,
                torch_dtype=torch.float16,
                device_map="auto",
                trust_remote_code=True
            )
            
            logging.info("âœ… ì›ë³¸ Llama-3.1-8B-Instruct ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
            
        except Exception as e:
            logging.error(f"ì›ë³¸ Llama ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            raise

    def _initialize_finetuned(self):
        """íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            self.finetuned_processor = get_finetuned_processor()
            logging.info("âœ… íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logging.error(f"íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            raise

    def _initialize_transformers(self):
        """Transformers ê¸°ë°˜ ì´ˆê¸°í™”"""
        # ëª¨ë¸ ë§¤ë‹ˆì € ê°€ì ¸ì˜¤ê¸°
        self.model_manager = get_model_manager()
        
        # ëª¨ë¸ë³„ í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
        self.processor = LLMProcessorFactory.create_processor(self.model_type)
        logging.info(f"Transformers LLM í”„ë¡œì„¸ì„œ ì´ˆê¸°í™” ì™„ë£Œ: {self.model_type}")
        
        # ëª¨ë¸ ë¡œë”©
        self._load_model_sync(self.model_type)

    def _initialize_llama_cpp(self):
        """llama-cpp-python ê¸°ë°˜ ì´ˆê¸°í™”"""
        try:
            # GGUF ëª¨ë¸ ê²½ë¡œ ì„¤ì •
            model_path = self._get_gguf_model_path()
            
            # llama-cpp í”„ë¡œì„¸ì„œ ì´ˆê¸°í™” (ê¸°ë³¸ ìŠ¤íƒ€ì¼: ì¹œê·¼í•œ ìƒë‹´ì‚¬)
            from .conversation_style_manager import ConversationStyle
            self.llama_cpp_processor = LlamaCppProcessor(model_path, self.model_type, ConversationStyle.FRIENDLY)
            logging.info(f"llama-cpp LLM í”„ë¡œì„¸ì„œ ì´ˆê¸°í™” ì™„ë£Œ: {self.model_type}")
            
        except Exception as e:
            logging.error(f"llama-cpp ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            # í´ë°±: transformers ì‚¬ìš©
            logging.info("Transformersë¡œ í´ë°±í•©ë‹ˆë‹¤.")
            self.use_llama_cpp = False
            self._initialize_transformers()

    def _get_gguf_model_path(self) -> str:
        """GGUF ëª¨ë¸ ê²½ë¡œ ë°˜í™˜ (Phi-3.5ë§Œ ì²´í¬)"""
        base_path = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(__file__)))), "models")
        model_path = os.path.join(base_path, "Phi-3.5-mini-instruct-Q8_0.gguf")
        
        if os.path.exists(model_path):
            return model_path
        
        raise FileNotFoundError(f"GGUF model not found: {model_path}")

    def _load_model_sync(self, model_type: str):
        """ëª¨ë¸ì„ ë™ê¸°ì ìœ¼ë¡œ ë¡œë”©í•©ë‹ˆë‹¤."""
        try:
            logging.info(f"Loading model: {model_type}")
            success = self.model_manager.load_model(model_type)
            if success:
                logging.info(f"Model {model_type} loaded successfully")
            else:
                logging.error(f"Failed to load model {model_type}")
        except Exception as e:
            logging.error(f"Failed to load model {model_type}: {str(e)}")
    
    def switch_model(self, model_type: str) -> bool:
        """ë‹¤ë¥¸ ëª¨ë¸ë¡œ ì „í™˜í•©ë‹ˆë‹¤."""
        try:
            if self.use_llama_cpp:
                # llama-cpp ëª¨ë¸ ì „í™˜
                model_path = self._get_gguf_model_path()
                self.llama_cpp_processor.cleanup()
                self.llama_cpp_processor = LlamaCppProcessor(model_path, model_type)
                self.model_type = model_type
                logging.info(f"Switched to llama-cpp model: {model_type}")
            else:
                # transformers ëª¨ë¸ ì „í™˜
                success = self.model_manager.switch_model(model_type)
                if success:
                    self.model_type = model_type
                    # í”„ë¡œì„¸ì„œë„ í•¨ê»˜ ì „í™˜
                    self.processor = LLMProcessorFactory.create_processor(model_type)
                    logging.info(f"Switched to transformers model: {model_type}")
                return success
            return True
        except Exception as e:
            logging.error(f"Failed to switch model: {str(e)}")
            return False
    
    def get_current_model(self):
        """í˜„ì¬ ëª¨ë¸ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        if self.use_llama_cpp:
            return self.llama_cpp_processor
        else:
            return self.model_manager.get_current_model()
    
    def get_current_model_config(self):
        """í˜„ì¬ ëª¨ë¸ ì„¤ì •ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        if self.use_llama_cpp:
            return {"type": "llama-cpp", "model_type": self.model_type}
        else:
            return self.model_manager.get_current_model_config()

    def inject_db_service(self, db_service):
        """DB ì„œë¹„ìŠ¤ë¥¼ ì™¸ë¶€ì—ì„œ ì£¼ì…ë°›ìŠµë‹ˆë‹¤."""
        self.search_service = db_service
        logging.info("DB ì„œë¹„ìŠ¤ ì£¼ì… ì™„ë£Œ")

    def remove_db_service(self):
        """DB ì„œë¹„ìŠ¤ë¥¼ ì œê±°í•©ë‹ˆë‹¤."""
        self.search_service = None
        logging.info("DB ì„œë¹„ìŠ¤ ì œê±° ì™„ë£Œ")

    def set_db_mode(self, enabled: bool):
        """DB ì—°ë™ ëª¨ë“œë¥¼ ì„¤ì •í•©ë‹ˆë‹¤."""
        self.use_db_mode = enabled
        logging.info(f"DB ì—°ë™ ëª¨ë“œ: {'í™œì„±í™”' if enabled else 'ë¹„í™œì„±í™”'}")
    
    def set_conversation_style(self, style: str):
        """ëŒ€í™” ìŠ¤íƒ€ì¼ì„ ì„¤ì •í•©ë‹ˆë‹¤."""
        try:
            from .conversation_style_manager import ConversationStyle
            style_enum = ConversationStyle(style)
            
            if self.use_llama_cpp and self.llama_cpp_processor:
                # ìƒˆë¡œìš´ ìŠ¤íƒ€ì¼ë¡œ í”„ë¡œì„¸ì„œ ì¬ì´ˆê¸°í™”
                model_path = self._get_gguf_model_path()
                self.llama_cpp_processor.cleanup()
                self.llama_cpp_processor = LlamaCppProcessor(model_path, self.model_type, style_enum)
                logging.info(f"ëŒ€í™” ìŠ¤íƒ€ì¼ ë³€ê²½ë¨: {style}")
            else:
                logging.warning("llama-cppê°€ í™œì„±í™”ë˜ì§€ ì•Šì•„ ìŠ¤íƒ€ì¼ ë³€ê²½ì´ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.")
                
        except ValueError:
            logging.error(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ìŠ¤íƒ€ì¼: {style}")
        except Exception as e:
            logging.error(f"ìŠ¤íƒ€ì¼ ë³€ê²½ ì¤‘ ì˜¤ë¥˜: {str(e)}")

    async def process_message(self, message: str, conversation_id: str = None) -> str:
        """
        ëª¨ë“ˆí™”ëœ ë©”ì‹œì§€ ì²˜ë¦¬ (DB ëª¨ë“œ/ìˆœìˆ˜ LLM ëª¨ë“œ ìë™ ì„ íƒ)
        
        Args:
            message: ì‚¬ìš©ì ë©”ì‹œì§€
            conversation_id: ëŒ€í™” ID
            
        Returns:
            str: ì²˜ë¦¬ëœ ì‘ë‹µ
        """
        start_time = time.time()
        self.response_stats['total_requests'] += 1
        
        try:
            # DB ëª¨ë“œê°€ í™œì„±í™”ë˜ê³  DB ì„œë¹„ìŠ¤ê°€ ìˆìœ¼ë©´ DB ì—°ë™ ì²˜ë¦¬
            if self.use_db_mode and self.search_service:
                response = await self._handle_db_enhanced_message(message)
            else:
                # ìˆœìˆ˜ LLM ëª¨ë“œ: ì›ë³¸ ëª¨ë¸ë§Œ ì‚¬ìš©
                response = await self._handle_pure_llm_message(message)
            
            # ì‘ë‹µ í¬ë§·íŒ…
            formatted_response = await self.format_and_send_response(response)
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            end_time = time.time()
            processing_time = (end_time - start_time) * 1000  # ms
            self._update_stats(processing_time, True)
            
            return formatted_response
            
        except Exception as e:
            logging.error(f"ë©”ì‹œì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            self.response_stats['errors'] += 1
            
            # ì˜¤ë¥˜ ì²˜ë¦¬ ì‹œê°„ ê¸°ë¡
            end_time = time.time()
            processing_time = (end_time - start_time) * 1000
            self._update_stats(processing_time, False)
            
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ì¼ì‹œì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."

    async def _handle_pure_llm_message(self, message: str) -> str:
        """ìˆœìˆ˜ LLM ëª¨ë“œ: ì›ë³¸ ëª¨ë¸ë§Œ ì‚¬ìš©"""
        logging.info("ìˆœìˆ˜ LLM ëª¨ë“œë¡œ ë©”ì‹œì§€ ì²˜ë¦¬")
        return await self._handle_transformers_casual(message)

    async def _handle_db_enhanced_message(self, message: str) -> str:
        """DB ì—°ë™ ëª¨ë“œ: DB ê²€ìƒ‰ + LLM ê°•í™”"""
        logging.info("DB ì—°ë™ ëª¨ë“œë¡œ ë©”ì‹œì§€ ì²˜ë¦¬")
        
        try:
            # 1. DBì—ì„œ ê´€ë ¨ ë‹µë³€ ê²€ìƒ‰
            db_start_time = time.time()
            db_answer = await self.search_service.search_answer(message)
            db_end_time = time.time()
            db_processing_time = (db_end_time - db_start_time) * 1000
            
            self.response_stats['db_processing_time'] += db_processing_time
            
            if db_answer:
                self.response_stats['db_responses'] += 1
                logging.info(f"DBì—ì„œ ë‹µë³€ ì°¾ìŒ: {db_answer[:100]}...")
                
                # 2. LLMìœ¼ë¡œ ë‹µë³€ ê°•í™”
                enhanced_answer = await self._enhance_db_answer_with_llm(message, db_answer)
                return enhanced_answer
            else:
                logging.info("DBì—ì„œ ê´€ë ¨ ë‹µë³€ì„ ì°¾ì§€ ëª»í•¨")
                # DBì—ì„œ ë‹µë³€ì„ ì°¾ì§€ ëª»í•œ ê²½ìš° ìˆœìˆ˜ LLMìœ¼ë¡œ ì²˜ë¦¬
                return await self._handle_pure_llm_message(message)
                
        except Exception as e:
            logging.error(f"DB ì—°ë™ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ìˆœìˆ˜ LLMìœ¼ë¡œ í´ë°±
            return await self._handle_pure_llm_message(message)

    async def get_conversation_response(self, message: str) -> str:
        """
        ëŒ€í™” ì‘ë‹µ ìƒì„± (íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ìš°ì„ )
        
        Args:
            message: ì‚¬ìš©ì ë©”ì‹œì§€
            
        Returns:
            str: ìƒì„±ëœ ì‘ë‹µ
        """
        try:
            # íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ìš°ì„  ì‚¬ìš©
            if self.use_finetuned and hasattr(self, 'finetuned_processor') and self.finetuned_processor:
                return await self._handle_finetuned_casual(message)
            elif self.use_llama_cpp:
                return await self._handle_llama_cpp_casual(message)
            else:
                return await self._handle_transformers_casual(message)
        except Exception as e:
            logging.error(f"ëŒ€í™” ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    async def _handle_finetuned_casual(self, message: str) -> str:
        """íŒŒì¸íŠœë‹ëœ ëª¨ë¸ë¡œ ì¼ìƒ ëŒ€í™” ì²˜ë¦¬"""
        try:
            if hasattr(self, 'finetuned_processor') and self.finetuned_processor:
                # íŒŒì¸íŠœë‹ëœ ëª¨ë¸ë¡œ ì‘ë‹µ ìƒì„±
                prompt = f"ì‚¬ìš©ì: {message}\nìƒë‹´ì‚¬:"
                response = self.finetuned_processor.generate_response(prompt, max_length=256, temperature=0.7)
                self.response_stats['finetuned_responses'] += 1
                return response
            else:
                # í´ë°±: ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©
                return await self._handle_llama_cpp_casual(message)
        except Exception as e:
            logging.error(f"íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            # í´ë°±: ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©
            return await self._handle_llama_cpp_casual(message)

    async def _handle_llama_cpp_casual(self, message: str) -> str:
        """llama-cppë¥¼ ì‚¬ìš©í•œ ì¼ìƒ ëŒ€í™” ì²˜ë¦¬"""
        try:
            # ì…ë ¥ í•„í„° ì²˜ë¦¬ (ìš•ì„¤, ë¹„ìƒë‹´ ì§ˆë¬¸ ì²´í¬)
            template_response, use_template = self.llama_cpp_processor.process_user_input(message)
            
            if use_template:
                # í…œí”Œë¦¿ ì‘ë‹µ ì‚¬ìš© (ìš•ì„¤, ë¹„ìƒë‹´ ì§ˆë¬¸)
                self.response_stats['llama_responses'] += 1
                return template_response
            
            # ì¼ë°˜ì ì¸ ê²½ìš° LLM ì²˜ë¦¬
            prompt = self.llama_cpp_processor.create_casual_prompt(message)
            response = self.llama_cpp_processor.generate_response(prompt)
            
            if response:
                self.response_stats['llama_responses'] += 1
                return response
            else:
                return "ì£„ì†¡í•©ë‹ˆë‹¤. ì ì ˆí•œ ì‘ë‹µì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
                
        except Exception as e:
            logging.error(f"llama-cpp ì¼ìƒ ëŒ€í™” ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    async def _handle_transformers_casual(self, message: str) -> str:
        """ì›ë³¸ Llama-3.1-8B-Instruct ëª¨ë¸ì„ ì‚¬ìš©í•œ ì¼ìƒ ëŒ€í™” ì²˜ë¦¬ (Hugging Face ê³µì‹ ë°©ì‹)"""
        try:
            # Hugging Face ê³µì‹ chat template ì‚¬ìš©
            messages = [
                {"role": "user", "content": message}
            ]
            
            # ê³µì‹ chat template ì ìš©
            formatted_prompt = self.tokenizer.apply_chat_template(
                messages, 
                tokenize=False, 
                add_generation_prompt=True
            )
            
            # í† í¬ë‚˜ì´ì§•
            inputs = self.tokenizer(formatted_prompt, return_tensors="pt")
            
            # ì›ë˜ ì˜ ë˜ë˜ ì„¤ì •ìœ¼ë¡œ ë³µì›
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs.input_ids,
                    max_new_tokens=30,         # 75 -> 30ìœ¼ë¡œ ë³µì› (ê°„ê²°í•œ ì‘ë‹µ)
                    temperature=0.7,           # 0.3 -> 0.7ë¡œ ë³µì› (ìì—°ìŠ¤ëŸ¬ì›€)
                    top_p=0.9,                 # 0.8 -> 0.9ë¡œ ë³µì›
                    do_sample=True,            # ìƒ˜í”Œë§ í™œì„±í™”
                    repetition_penalty=1.1,    # 1.2 -> 1.1ë¡œ ë³µì›
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )
            
            # ì‘ë‹µ ë””ì½”ë”© (ê³µì‹ ë°©ì‹)
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ ì œê±°í•˜ê³  ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µë§Œ ì¶”ì¶œ
            # Hugging Face ê³µì‹ chat template í˜•ì‹ì— ë§ì¶° ì²˜ë¦¬
            if "<|im_start|>assistant" in response:
                assistant_response = response.split("<|im_start|>assistant")[-1].strip()
            elif "assistant" in response:
                # ë‹¤ë¥¸ í˜•ì‹ì˜ assistant íƒœê·¸ ì²˜ë¦¬
                assistant_response = response.split("assistant")[-1].strip()
            else:
                # í”„ë¡¬í”„íŠ¸ ì œê±°
                assistant_response = response.replace(formatted_prompt, "").strip()
            
            # íŠ¹ìˆ˜ í† í° ì œê±°
            assistant_response = assistant_response.replace("<|im_end|>", "").replace("<|im_start|>", "")
            assistant_response = assistant_response.replace("<|endoftext|>", "")
            
            # ì¸ì‚¬ë§ ê°„ê²°í™”
            if any(word in message.lower() for word in ['ì•ˆë…•', 'í•˜ì´', 'ë°˜ê°‘']):
                return "ì•ˆë…•í•˜ì„¸ìš”! ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”?"
            
            if assistant_response:
                self.response_stats['llama_responses'] += 1
                return assistant_response
            else:
                return "ì–´ë–¤ ë„ì›€ì´ í•„ìš”í•˜ì‹ ê°€ìš”?"
                
        except Exception as e:
            logging.error(f"ì›ë³¸ Llama ì¼ìƒ ëŒ€í™” ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    async def search_and_enhance_answer(self, message: str) -> str:
        """
        DB ê²€ìƒ‰ í›„ LLMìœ¼ë¡œ ë‹µë³€ ê°•í™”
        
        Args:
            message: ì‚¬ìš©ì ë©”ì‹œì§€
            
        Returns:
            str: ê°•í™”ëœ ë‹µë³€
        """
        logging.info(f"ğŸ” search_and_enhance_answer ì‹œì‘: {message[:50]}...")
        
        if not self.search_service:
            logging.warning("âŒ Search service not available")
            return await self.get_conversation_response(message)
        
        try:
            # 1. DBì—ì„œ ê´€ë ¨ ë‹µë³€ ê²€ìƒ‰
            logging.info("ğŸ” DB ê²€ìƒ‰ ì‹œì‘")
            db_start_time = time.time()
            db_answer = await self.search_service.search_answer(message)
            db_end_time = time.time()
            db_processing_time = (db_end_time - db_start_time) * 1000
            
            self.response_stats['db_processing_time'] += db_processing_time
            
            if db_answer:
                self.response_stats['db_responses'] += 1
                logging.info(f"âœ… DBì—ì„œ ë‹µë³€ ì°¾ìŒ: {db_answer[:100]}...")
                
                # 2. LLMìœ¼ë¡œ ë‹µë³€ ê°•í™”
                logging.info("ğŸ” LLMìœ¼ë¡œ ë‹µë³€ ê°•í™” ì‹œì‘")
                enhanced_answer = await self._enhance_db_answer_with_llm(message, db_answer)
                logging.info(f"âœ… LLM ê°•í™” ì™„ë£Œ: {enhanced_answer[:100]}...")
                return enhanced_answer
            else:
                logging.info("âŒ DBì—ì„œ ê´€ë ¨ ë‹µë³€ì„ ì°¾ì§€ ëª»í•¨")
                # DBì—ì„œ ë‹µë³€ì„ ì°¾ì§€ ëª»í•œ ê²½ìš° ì¼ë°˜ ëŒ€í™”ë¡œ ì²˜ë¦¬
                logging.info("ğŸ” ì¼ë°˜ ëŒ€í™”ë¡œ í´ë°±")
                return await self.get_conversation_response(message)
                
        except Exception as e:
            logging.error(f"âŒ DB ê²€ìƒ‰ ë° ê°•í™” ì¤‘ ì˜¤ë¥˜: {str(e)}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì¼ë°˜ ëŒ€í™”ë¡œ í´ë°±
            return await self.get_conversation_response(message)

    async def format_and_send_response(self, response: str) -> str:
        """ì‘ë‹µ í¬ë§·íŒ… ë° ì „ì†¡"""
        try:
            # ê¸°ë³¸ í¬ë§·íŒ…
            formatted_response = self._format_response(response)
            
            # ì¶”ê°€ í¬ë§·íŒ… ì„œë¹„ìŠ¤ ì‚¬ìš©
            formatting_service = FormattingService()
            final_response = formatting_service.format_response(formatted_response)
            
            return final_response
            
        except Exception as e:
            logging.error(f"ì‘ë‹µ í¬ë§·íŒ… ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return response

    async def _handle_casual_conversation(self, message: str) -> str:
        """ì¼ìƒ ëŒ€í™” ì²˜ë¦¬"""
        return await self.get_conversation_response(message)

    async def _handle_professional_conversation(self, message: str) -> str:
        """ì „ë¬¸ ìƒë‹´ ì²˜ë¦¬"""
        return await self.search_and_enhance_answer(message)

    async def _enhance_db_answer_with_llm(self, message: str, db_answer: str) -> str:
        """DB ë‹µë³€ì„ ì›ë³¸ Llama-3.1-8B-Instruct ëª¨ë¸ë¡œ ê°•í™”"""
        try:
            # LLaMA í˜•ì‹ í”„ë¡¬í”„íŠ¸ (ë” ê°„ê²°í•˜ê³  ì •í™•í•˜ê²Œ)
            formatted_prompt = f"""<|im_start|>user
ì‚¬ìš©ì ì§ˆë¬¸: {message}

DB ë‹µë³€: {db_answer}

ìœ„ DB ë‹µë³€ì„ ë°”íƒ•ìœ¼ë¡œ ê°„ê²°í•˜ê³  ì •í™•í•œ í•´ê²° ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”. DB ë‹µë³€ì˜ í•µì‹¬ ë‚´ìš©ì„ ìœ ì§€í•˜ë©´ì„œ ì¹œì ˆí•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”. ë¶ˆí•„ìš”í•œ í™•ì¥ì€ í•˜ì§€ ë§ˆì„¸ìš”.<|im_end|>
<|im_start|>assistant
"""
            
            # í† í¬ë‚˜ì´ì§•
            inputs = self.tokenizer(formatted_prompt, return_tensors="pt")
            
            # ì•ˆì •ì ì¸ ì‘ë‹µì„ ìœ„í•œ ì„¤ì •
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs.input_ids,
                    max_new_tokens=500,         # 300 -> 500ìœ¼ë¡œ ì¦ê°€ (ì™„ì „í•œ ë‹µë³€ ë³´ì¥)
                    temperature=0.7,           # ìì—°ìŠ¤ëŸ¬ì›€ ìœ ì§€
                    top_p=0.9,                 # ì•ˆì •ì„±
                    do_sample=True,
                    repetition_penalty=1.1,    # ë°˜ë³µ ë°©ì§€
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    early_stopping=True,       # ì¡°ê¸° ì¢…ë£Œ í™œì„±í™”
                    num_beams=1               # ë‹¨ì¼ ë¹”ìœ¼ë¡œ ì†ë„ í–¥ìƒ
                )
            
            # ì‘ë‹µ ë””ì½”ë”©
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ ì œê±°í•˜ê³  ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µë§Œ ì¶”ì¶œ
            if "<|im_start|>assistant" in response:
                assistant_response = response.split("<|im_start|>assistant")[-1].strip()
            else:
                assistant_response = response.replace(formatted_prompt, "").strip()
            
            # LLaMA íŠ¹ìˆ˜ í† í° ì œê±° (ë” ì² ì €í•˜ê²Œ)
            assistant_response = assistant_response.replace("<|im_end|>", "").replace("<|im_start|>", "")
            assistant_response = assistant_response.replace("<|fim_end|>", "").replace("<|fim_start|>", "")
            assistant_response = assistant_response.replace("<|endoftext|>", "").replace("<|eom_complete|>", "")
            
            # ì´ìƒí•œ í† í° íŒ¨í„´ ì œê±°
            import re
            assistant_response = re.sub(r'\|<\|[^>]+\|>', '', assistant_response)
            assistant_response = re.sub(r'<\|[^>]+\|>', '', assistant_response)
            
            # ì‘ë‹µì´ ë„ˆë¬´ ì§§ìœ¼ë©´ DB ë‹µë³€ ê·¸ëŒ€ë¡œ ë°˜í™˜
            if len(assistant_response.strip()) < 20:
                logging.warning("LLM ì‘ë‹µì´ ë„ˆë¬´ ì§§ìŒ, DB ë‹µë³€ ì‚¬ìš©")
                return self._format_db_answer(db_answer)
            
            if assistant_response:
                self.response_stats['llama_responses'] += 1
                return assistant_response
            
            # LLM ê°•í™” ì‹¤íŒ¨ ì‹œ DB ë‹µë³€ ê·¸ëŒ€ë¡œ ë°˜í™˜
            return self._format_db_answer(db_answer)
                
        except Exception as e:
            logging.error(f"DB ë‹µë³€ LLM ê°•í™” ì¤‘ ì˜¤ë¥˜: {str(e)}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ DB ë‹µë³€ ê·¸ëŒ€ë¡œ ë°˜í™˜
            return self._format_db_answer(db_answer)

    def _format_db_answer(self, db_answer: str) -> str:
        """DB ë‹µë³€ í¬ë§·íŒ… (ë©”íƒ€ë°ì´í„° ë° ë¶ˆí•„ìš”í•œ ë¬¸ì ì œê±°)"""
        try:
            # ê¸°ë³¸ ì •ë¦¬
            formatted = db_answer.strip()
            
            # ë©”íƒ€ë°ì´í„° íŒ¨í„´ ì œê±° (ë‚ ì§œ, ì‹œê°„, í† í° ë“±)
            import re
            
            # ë‚ ì§œ/ì‹œê°„ íŒ¨í„´ ì œê±°: | 2024-03-18 02:23:13 |
            formatted = re.sub(r'\|\s*\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2}:\d{2}\s*\|', '', formatted)
            
            # LLaMA íŠ¹ìˆ˜ í† í° ì œê±°
            formatted = re.sub(r'<\|im_end\|>', '', formatted)
            formatted = re.sub(r'<\|im_start\|>', '', formatted)
            formatted = re.sub(r'<\|endoftext\|>', '', formatted)
            
            # ê¸°íƒ€ íŠ¹ìˆ˜ ë¬¸ì íŒ¨í„´ ì œê±°
            formatted = re.sub(r'\|\s*\|', '', formatted)  # ë¹ˆ íŒŒì´í”„ ì œê±°
            formatted = re.sub(r'^\|\s*', '', formatted)   # ì‹œì‘ íŒŒì´í”„ ì œê±°
            formatted = re.sub(r'\s*\|$', '', formatted)   # ë íŒŒì´í”„ ì œê±°
            
            # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ
            formatted = re.sub(r'\s+', ' ', formatted)
            
            # ì•ë’¤ ê³µë°± ì œê±°
            formatted = formatted.strip()
            
            # ê¸¸ì´ ì œí•œ ì œê±° - ì™„ì „í•œ ë‹µë³€ ë³´ì¥
            # if len(formatted) > 1000:
            #     formatted = formatted[:1000] + "..."
            
            return formatted
            
        except Exception as e:
            logging.error(f"DB ë‹µë³€ í¬ë§·íŒ… ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return db_answer

    def _format_response(self, response: str) -> str:
        """ì‘ë‹µ í¬ë§·íŒ…"""
        if not response:
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
        
        # ê¸°ë³¸ ì •ë¦¬
        response = response.strip()
        
        # ê¸¸ì´ ì œí•œ (ë” ì§§ê²Œ)
        if len(response) > 500:
            response = response[:500] + "..."
        
        return response

    def _update_stats(self, processing_time: float, success: bool):
        """í†µê³„ ì—…ë°ì´íŠ¸"""
        self.response_stats['total_processing_time'] += processing_time
        self.response_stats['processing_times'].append(processing_time)
        
        # ìµœì†Œ/ìµœëŒ€ ì²˜ë¦¬ ì‹œê°„ ì—…ë°ì´íŠ¸
        if processing_time < self.response_stats['min_processing_time']:
            self.response_stats['min_processing_time'] = processing_time
        if processing_time > self.response_stats['max_processing_time']:
            self.response_stats['max_processing_time'] = processing_time
        
        if not success:
            self.response_stats['error_processing_time'] += processing_time

    def get_response_stats(self) -> Dict:
        """ì‘ë‹µ í†µê³„ ë°˜í™˜"""
        stats = self.response_stats.copy()
        
        # í‰ê·  ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
        if stats['processing_times']:
            stats['avg_processing_time'] = sum(stats['processing_times']) / len(stats['processing_times'])
        else:
            stats['avg_processing_time'] = 0
        
        # ì„±ê³µë¥  ê³„ì‚°
        if stats['total_requests'] > 0:
            stats['success_rate'] = (stats['total_requests'] - stats['errors']) / stats['total_requests'] * 100
        else:
            stats['success_rate'] = 0
        
        return stats

    def log_response_stats(self):
        """ì‘ë‹µ í†µê³„ ë¡œê¹…"""
        stats = self.get_response_stats()
        
        logging.info("=== LLM ì„œë¹„ìŠ¤ ì‘ë‹µ í†µê³„ ===")
        logging.info(f"ì´ ìš”ì²­ ìˆ˜: {stats['total_requests']}")
        logging.info(f"ì¼ìƒ ëŒ€í™”: {stats['casual_conversations']}")
        logging.info(f"ì „ë¬¸ ìƒë‹´: {stats['professional_conversations']}")
        logging.info(f"DB ì‘ë‹µ: {stats['db_responses']}")
        logging.info(f"LLM ì‘ë‹µ: {stats['llama_responses']}")
        logging.info(f"ì˜¤ë¥˜: {stats['errors']}")
        logging.info(f"ì„±ê³µë¥ : {stats['success_rate']:.1f}%")
        logging.info(f"í‰ê·  ì²˜ë¦¬ ì‹œê°„: {stats['avg_processing_time']:.2f}ms")
        logging.info(f"ìµœì†Œ ì²˜ë¦¬ ì‹œê°„: {stats['min_processing_time']:.2f}ms")
        logging.info(f"ìµœëŒ€ ì²˜ë¦¬ ì‹œê°„: {stats['max_processing_time']:.2f}ms")
        logging.info(f"ì´ ì²˜ë¦¬ ì‹œê°„: {stats['total_processing_time']:.2f}ms")
        logging.info(f"DB ì²˜ë¦¬ ì‹œê°„: {stats['db_processing_time']:.2f}ms")
        logging.info(f"LLM ì²˜ë¦¬ ì‹œê°„: {stats['llama_processing_time']:.2f}ms")
        logging.info(f"ì˜¤ë¥˜ ì²˜ë¦¬ ì‹œê°„: {stats['error_processing_time']:.2f}ms")
        logging.info("================================")