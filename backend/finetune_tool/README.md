# AI μƒλ‹΄μ‚¬ νμΈνλ‹ λ„κµ¬

Phi-3.5 3.8B λ¨λΈμ„ μƒλ‹΄μ‚¬ νΉν™” λ¨λΈλ΅ νμΈνλ‹ν•λ” λ„κµ¬μ…λ‹λ‹¤.

## π“‹ λ©μ°¨

- [κ°μ”](#κ°μ”)
- [μ‹μ¤ν… μ”κµ¬μ‚¬ν•­](#μ‹μ¤ν…-μ”κµ¬μ‚¬ν•­)
- [μ„¤μΉ](#μ„¤μΉ)
- [μ‚¬μ©λ²•](#μ‚¬μ©λ²•)
- [ν΄λΌμ°λ“ GPU μ§€μ›](#ν΄λΌμ°λ“-gpu-μ§€μ›)
- [λ¬Έμ  ν•΄κ²°](#λ¬Έμ -ν•΄κ²°)

## π― κ°μ”

μ΄ λ„κµ¬λ” λ‹¤μκ³Ό κ°™μ€ κΈ°λ¥μ„ μ κ³µν•©λ‹λ‹¤:

- **λ°μ΄ν„° μ „μ²λ¦¬**: FAQ CSV λ°μ΄ν„°μ™€ λ€ν™” λ°μ΄ν„°λ¥Ό νμΈνλ‹ ν•μ‹μΌλ΅ λ³€ν™
- **LoRA νμΈνλ‹**: λ©”λ¨λ¦¬ ν¨μ¨μ μΈ νμΈνλ‹ (GTX 1050 Ti μ§€μ›)
- **GUI μΈν„°νμ΄μ¤**: μ‚¬μ©μ μΉν™”μ μΈ νμΈνλ‹ μ„¤μ • λ° λ¨λ‹ν„°λ§
- **λ¨λΈ ν…μ¤νΈ**: νμΈνλ‹λ λ¨λΈμ μ„±λ¥ ν‰κ°€
- **ν΄λΌμ°λ“ μ§€μ›**: ν–¥ν›„ AWS, GCP, Azure GPU μ„λ²„ μ§€μ› μμ •

## π’» μ‹μ¤ν… μ”κµ¬μ‚¬ν•­

### μµμ† μ”κµ¬μ‚¬ν•­ (GTX 1050 Ti)
- **GPU**: NVIDIA GTX 1050 Ti (4GB VRAM)
- **RAM**: 8GB
- **μ €μ¥κ³µκ°„**: 10GB
- **OS**: Windows 10/11, Linux, macOS

### κ¶μ¥ μ‚¬μ–‘
- **GPU**: RTX 3070 μ΄μƒ (8GB+ VRAM)
- **RAM**: 16GB+
- **μ €μ¥κ³µκ°„**: 20GB+

## π“¦ μ„¤μΉ

### 1. μ €μ¥μ† ν΄λ΅ 
```bash
cd backend/finetune_tool
```

### 2. μμ΅΄μ„± μ„¤μΉ
```bash
pip install -r requirements.txt
```

### 3. GPU λ“λΌμ΄λ²„ ν™•μΈ
```bash
nvidia-smi
```

## π€ μ‚¬μ©λ²•

### 1. νμΈνλ‹ μ‹¤ν–‰ (GPU μ»΄ν“¨ν„°)

#### GUI λ°©μ‹ (κ¶μ¥)
```bash
python run_finetune_gui.py
```

#### λ…λ Ήν–‰ λ°©μ‹
```bash
python -c "
from config import config_manager
from data_processor import DataProcessor
from model_trainer import ModelTrainer

# λ°μ΄ν„° μ²λ¦¬
processor = DataProcessor(config_manager)
train_dataset, val_dataset, test_dataset = processor.process_all_data()

# νμΈνλ‹
trainer = ModelTrainer(config_manager)
trainer.train(train_dataset, val_dataset)
trainer.save_model()
"
```

### 2. λ¨λΈ ν…μ¤νΈ (κ°λ° μ»΄ν“¨ν„°)

#### μ‹λ‚λ¦¬μ¤ ν…μ¤νΈ
```bash
python test_tool/run_model_test.py --model_path ./finetuned_models/finetuned_phi_20241201_143022 --test_type scenarios
```

#### λΉ„κµ ν…μ¤νΈ
```bash
python test_tool/run_model_test.py --model_path ./finetuned_models/finetuned_phi_20241201_143022 --test_type comparison --original_model microsoft/Phi-3.5-mini
```

#### λ€ν™”ν• ν…μ¤νΈ
```bash
python test_tool/run_model_test.py --model_path ./finetuned_models/finetuned_phi_20241201_143022 --test_type interactive
```

## βοΈ ν΄λΌμ°λ“ GPU μ§€μ›

### ν„μ¬ μ§€μ›
- **λ΅μ»¬ GPU**: GTX 1050 Ti μ΄μƒ

### ν–¥ν›„ μ§€μ› μμ •
- **AWS**: g4dn.xlarge, p3.2xlarge
- **GCP**: n1-standard-4 + Tesla T4
- **Azure**: Standard_NC4as_T4_v3

### ν΄λΌμ°λ“ μ‚¬μ© μ‹ μ›ν¬ν”λ΅μ°
1. λ°μ΄ν„°λ¥Ό ν΄λΌμ°λ“ μ¤ν† λ¦¬μ§€μ— μ—…λ΅λ“
2. ν΄λΌμ°λ“ GPU μΈμ¤ν„΄μ¤μ—μ„ νμΈνλ‹ μ‹¤ν–‰
3. νμΈνλ‹λ λ¨λΈμ„ λ΅μ»¬λ΅ λ‹¤μ΄λ΅λ“
4. λ΅μ»¬μ—μ„ ν…μ¤νΈ λ° μ μ©

## β™οΈ μ„¤μ •

### κΈ°λ³Έ μ„¤μ • νμΌ
```python
# config.pyμ—μ„ μμ • κ°€λ¥
finetune_config = {
    "model_name": "microsoft/Phi-3.5-mini",
    "lora_r": 16,
    "lora_alpha": 32,
    "num_epochs": 3,
    "learning_rate": 2e-4,
    "batch_size": 1,  # GTX 1050 Tiμ©
    "use_4bit": True  # λ©”λ¨λ¦¬ μ μ•½
}
```

### ν•λ“μ›¨μ–΄λ³„ κ¶μ¥ μ„¤μ •

#### GTX 1050 Ti (4GB)
```python
{
    "batch_size": 1,
    "gradient_accumulation_steps": 16,
    "use_4bit": True,
    "max_seq_length": 512
}
```

#### RTX 3070 (8GB)
```python
{
    "batch_size": 2,
    "gradient_accumulation_steps": 8,
    "use_4bit": True,
    "max_seq_length": 1024
}
```

#### RTX 4090 (24GB)
```python
{
    "batch_size": 8,
    "gradient_accumulation_steps": 2,
    "use_4bit": False,
    "max_seq_length": 2048
}
```

## π“ μ„±λ¥ μμƒ

### GTX 1050 Ti κΈ°μ¤€
- **λ°μ΄ν„°**: 300κ° FAQ + λ€ν™” λ°μ΄ν„°
- **μμƒ μ‹κ°„**: 2-4μ‹κ°„
- **λ©”λ¨λ¦¬ μ‚¬μ©λ‰**: 4-6GB
- **λ¨λΈ ν¬κΈ°**: ~100MB (LoRA μ–΄λ‘ν„°)

### μ„±λ¥ κ°μ„  μμƒ
- **ν‚¤μ›λ“ λ§¤μΉ­λ¥ **: 60-80%
- **μ‘λ‹µ ν’μ§**: 40-60% κ°μ„ 
- **μƒλ‹΄ μ „λ¬Έμ„±**: 50-70% ν–¥μƒ

## π”§ λ¬Έμ  ν•΄κ²°

### μΌλ°μ μΈ λ¬Έμ 

#### 1. CUDA λ©”λ¨λ¦¬ λ¶€μ΅±
```
RuntimeError: CUDA out of memory
```
**ν•΄κ²°μ±…**:
- `batch_size`λ¥Ό 1λ΅ μ„¤μ •
- `gradient_accumulation_steps`λ¥Ό 16μΌλ΅ μ¦κ°€
- `use_4bit: True` μ„¤μ •

#### 2. ν¨ν‚¤μ§€ μ„¤μΉ μ¤λ¥
```
ImportError: No module named 'transformers'
```
**ν•΄κ²°μ±…**:
```bash
pip install -r requirements.txt
```

#### 3. λ¨λΈ λ΅λ“ μ‹¤ν¨
```
OSError: Can't load tokenizer
```
**ν•΄κ²°μ±…**:
- μΈν„°λ„· μ—°κ²° ν™•μΈ
- Hugging Face ν† ν° μ„¤μ •
- λ¨λΈ κ²½λ΅ ν™•μΈ

### λ΅κ·Έ ν™•μΈ
```bash
# μƒμ„Έ λ΅κ·Έ λ³΄κΈ°
python run_finetune_gui.py --verbose

# λ΅κ·Έ νμΌ ν™•μΈ
tail -f finetune.log
```

## π“ νμΌ κµ¬μ΅°

```
finetune_tool/
β”β”€β”€ config.py              # μ„¤μ • κ΄€λ¦¬
β”β”€β”€ data_processor.py      # λ°μ΄ν„° μ „μ²λ¦¬
β”β”€β”€ model_trainer.py       # λ¨λΈ ν›λ ¨
β”β”€β”€ requirements.txt       # μμ΅΄μ„±
β”β”€β”€ run_finetune_gui.py    # GUI μ‹¤ν–‰
β”β”€β”€ gui/
β”‚   β””β”€β”€ main_window.py     # GUI λ©”μΈ μλ„μ°
β””β”€β”€ test_tool/
    β”β”€β”€ model_tester.py    # λ¨λΈ ν…μ¤νΈ
    β””β”€β”€ run_model_test.py  # ν…μ¤νΈ μ‹¤ν–‰
```

## π¤ κΈ°μ—¬

λ²„κ·Έ λ¦¬ν¬νΈλ‚ κΈ°λ¥ μ”μ²­μ€ μ΄μλ΅ λ“±λ΅ν•΄μ£Όμ„Έμ”.

## π“„ λΌμ΄μ„ μ¤

μ΄ ν”„λ΅μ νΈλ” MIT λΌμ΄μ„ μ¤ ν•μ— λ°°ν¬λ©λ‹λ‹¤. 