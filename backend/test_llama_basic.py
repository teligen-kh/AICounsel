#!/usr/bin/env python3
"""
LLaMA 2 7B Chat ëª¨ë¸ ê¸°ë³¸ í…ŒìŠ¤íŠ¸
"""

import os
import sys
import time
from llama_cpp import Llama

def test_llama_basic():
    """LLaMA 2 7B ëª¨ë¸ ê¸°ë³¸ í…ŒìŠ¤íŠ¸"""
    
    # ëª¨ë¸ ê²½ë¡œ
    model_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), "models", "llama-2-7b-chat.Q4_K_M.gguf")
    
    if not os.path.exists(model_path):
        print(f"âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
        return False
    
    print(f"âœ… ëª¨ë¸ íŒŒì¼ ë°œê²¬: {model_path}")
    
    try:
        # ëª¨ë¸ ì´ˆê¸°í™”
        print("ğŸ”„ ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
        start_time = time.time()
        
        llm = Llama(
            model_path=model_path,
            n_ctx=2048,
            n_threads=8,
            n_gpu_layers=0,
            verbose=False,
            seed=42
        )
        
        init_time = time.time() - start_time
        print(f"âœ… ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ: {init_time:.2f}ì´ˆ")
        
        # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë“¤
        test_cases = [
            ("ì•ˆë…•í•˜ì„¸ìš”", "í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”."),
            ("ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ë„¤ìš”", "í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”."),
            ("í•œêµ­ì–´ë¡œ ëŒ€í™”í•´ì£¼ì„¸ìš”", ""),
            ("1+1ì€ ëª‡ì¸ê°€ìš”?", "í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”."),
            ("ì¸ê³µì§€ëŠ¥ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”", "í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.")
        ]
        
        print("\n" + "="*50)
        print("LLaMA 2 7B Chat ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        print("="*50)
        
        for i, (test_input, instruction) in enumerate(test_cases, 1):
            print(f"\n--- í…ŒìŠ¤íŠ¸ {i}: {test_input} ---")
            
            # í”„ë¡¬í”„íŠ¸ ìƒì„± (í•œêµ­ì–´ ì‘ë‹µ ìœ ë„)
            if instruction:
                prompt = f"<s>[INST] {test_input} {instruction} [/INST]"
            else:
                prompt = f"<s>[INST] {test_input} [/INST]"
            print(f"í”„ë¡¬í”„íŠ¸: {prompt}")
            
            # ì‘ë‹µ ìƒì„±
            start_time = time.time()
            
            response = llm(
                prompt,
                max_tokens=100,
                temperature=0.6,
                top_p=0.9,
                top_k=50,
                repeat_penalty=1.1,
                stop=["[INST]", "</s>"]
            )
            
            end_time = time.time()
            processing_time = (end_time - start_time) * 1000  # ms
            
            # ì‘ë‹µ ì¶”ì¶œ
            generated_text = response.get('choices', [{}])[0].get('text', '').strip()
            
            print(f"ì‘ë‹µ: {generated_text}")
            print(f"ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ms")
            print(f"ì‘ë‹µ ê¸¸ì´: {len(generated_text)} ë¬¸ì")
            
            # ì‘ë‹µ í’ˆì§ˆ í‰ê°€
            if len(generated_text) < 5:
                print("âš ï¸  ì‘ë‹µì´ ë„ˆë¬´ ì§§ìŒ")
            elif processing_time > 10000:  # 10ì´ˆ
                print("âš ï¸  ì²˜ë¦¬ ì‹œê°„ì´ ë„ˆë¬´ ê¹€")
            elif not any(char in generated_text for char in ['ì•ˆë…•', 'ë„¤', 'ì¢‹', 'ê°ì‚¬', 'ë„ì›€', 'ì„¤ëª…', 'ë‹µë³€']):
                print("âš ï¸  ì‘ë‹µì´ ë¶€ì ì ˆí•¨")
            else:
                print("âœ… ì‘ë‹µ í’ˆì§ˆ ì–‘í˜¸")
        
        print("\n" + "="*50)
        print("í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        print("="*50)
        
        return True
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = test_llama_basic()
    if success:
        print("\nâœ… LLaMA 2 7B ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì„±ê³µ")
    else:
        print("\nâŒ LLaMA 2 7B ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
        sys.exit(1) 