#!/usr/bin/env python3
"""
ì „ì²´ technical í‚¤ì›Œë“œ ì¶”ì¶œ ë° context_patterns ì €ì¥ ëª¨ë“ˆ
1. knowledge_baseì˜ ëª¨ë“  ì§ˆë¬¸ ê°€ì ¸ì˜¤ê¸°
2. LLMìœ¼ë¡œ technical í‚¤ì›Œë“œ ì¶”ì¶œ
3. ì¤‘ë³µ ì œê±°
4. context_patterns í…Œì´ë¸”ì— ì €ì¥
"""

import asyncio
import motor.motor_asyncio
import logging
from typing import List, Dict, Set
import sys
import os
from datetime import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
current_dir = os.path.dirname(os.path.abspath(__file__))
backend_dir = os.path.dirname(os.path.dirname(current_dir))
sys.path.insert(0, backend_dir)

from app.database import get_database
from app.services.llm_service import LLMService

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class FullTechnicalKeywordExtractor:
    """ì „ì²´ technical í‚¤ì›Œë“œ ì¶”ì¶œê¸°"""
    
    def __init__(self, db, llm_service):
        self.db = db
        self.knowledge_collection = db.knowledge_base
        self.pattern_collection = db.context_patterns
        self.llm_service = llm_service
        
    async def get_all_questions(self) -> List[str]:
        """knowledge_baseì—ì„œ ëª¨ë“  ì§ˆë¬¸ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        try:
            logger.info("knowledge_baseì—ì„œ ëª¨ë“  ì§ˆë¬¸ì„ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
            
            # ëª¨ë“  ì§ˆë¬¸ ê°€ì ¸ì˜¤ê¸°
            all_questions = await self.knowledge_collection.find({}, {"question": 1}).to_list(length=None)
            
            if not all_questions:
                logger.error("knowledge_baseì— ì§ˆë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")
                return []
            
            questions = [q['question'] for q in all_questions]
            logger.info(f"ì´ {len(questions)}ê°œì˜ ì§ˆë¬¸ì„ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.")
            
            return questions
            
        except Exception as e:
            logger.error(f"ì§ˆë¬¸ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return []
    
    async def extract_technical_keywords_with_llm(self, question: str) -> List[str]:
        """LLMì„ ì‚¬ìš©í•´ì„œ ì§ˆë¬¸ì—ì„œ technical í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        try:
            prompt = f"""
ë‹¤ìŒ ì§ˆë¬¸ì—ì„œ ê¸°ìˆ ì  ë¬¸ì œë‚˜ ìƒë‹´ê³¼ ê´€ë ¨ëœ í•µì‹¬ í‚¤ì›Œë“œë“¤ì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”.
í‚¤ì›Œë“œëŠ” 2-4ê°œ ì •ë„ë¡œ ì¶”ì¶œí•˜ê³ , ì‰¼í‘œë¡œ êµ¬ë¶„í•´ì„œ ë‹µë³€í•´ì£¼ì„¸ìš”.
ê¸°ìˆ ì  ë¬¸ì œë‚˜ ìƒë‹´ê³¼ ê´€ë ¨ ì—†ëŠ” ì¼ë°˜ì ì¸ ë‹¨ì–´ëŠ” ì œì™¸í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {question}

ì¶”ì¶œëœ í‚¤ì›Œë“œë“¤:
"""
            
            response = await self.llm_service._handle_pure_llm_message(prompt)
            
            # ì‘ë‹µì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
            keywords = self._parse_keywords_from_response(response)
            
            return keywords
            
        except Exception as e:
            logger.error(f"LLM í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    def _parse_keywords_from_response(self, response: str) -> List[str]:
        """LLM ì‘ë‹µì—ì„œ í‚¤ì›Œë“œë¥¼ íŒŒì‹±í•©ë‹ˆë‹¤."""
        try:
            # ì‘ë‹µ ì •ë¦¬
            response = response.strip()
            
            # ì‰¼í‘œë¡œ ë¶„ë¦¬
            keywords = [kw.strip() for kw in response.split(',')]
            
            # ë¹ˆ ë¬¸ìì—´ ì œê±°
            keywords = [kw for kw in keywords if kw]
            
            # 2-4ê°œë¡œ ì œí•œ
            return keywords[:4]
            
        except Exception as e:
            logger.error(f"í‚¤ì›Œë“œ íŒŒì‹± ì‹¤íŒ¨: {e}")
            return []
    
    async def process_all_questions(self, questions: List[str]) -> List[Dict]:
        """ëª¨ë“  ì§ˆë¬¸ì„ ì²˜ë¦¬í•˜ì—¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        logger.info("=== ëª¨ë“  ì§ˆë¬¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹œì‘ ===")
        
        results = []
        total_questions = len(questions)
        
        for i, question in enumerate(questions, 1):
            logger.info(f"ì²˜ë¦¬ ì¤‘: {i}/{total_questions} - {question[:50]}...")
            
            keywords = await self.extract_technical_keywords_with_llm(question)
            
            if keywords:
                results.append({
                    'question': question,
                    'keywords': keywords
                })
            
            # ì§„í–‰ë¥  í‘œì‹œ
            if i % 10 == 0:
                logger.info(f"ì§„í–‰ë¥ : {i}/{total_questions} ({i/total_questions*100:.1f}%)")
            
            # LLM ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ëŒ€ê¸°
            await asyncio.sleep(0.5)
        
        logger.info(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ: {len(results)}ê°œ ì§ˆë¬¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œë¨")
        return results
    
    def remove_duplicates(self, results: List[Dict]) -> Set[str]:
        """ì¤‘ë³µì„ ì œê±°í•˜ê³  ê³ ìœ í•œ í‚¤ì›Œë“œë“¤ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        logger.info("ì¤‘ë³µ ì œê±° ì¤‘...")
        
        all_keywords = set()
        for result in results:
            all_keywords.update(result['keywords'])
        
        logger.info(f"ì¤‘ë³µ ì œê±° ì™„ë£Œ: {len(all_keywords)}ê°œì˜ ê³ ìœ  í‚¤ì›Œë“œ")
        return all_keywords
    
    async def save_to_context_patterns(self, keywords: Set[str]) -> bool:
        """ì¶”ì¶œëœ í‚¤ì›Œë“œë“¤ì„ context_patterns í…Œì´ë¸”ì— ì €ì¥í•©ë‹ˆë‹¤."""
        try:
            logger.info("context_patterns í…Œì´ë¸”ì— ì €ì¥ ì¤‘...")
            
            # ê¸°ì¡´ ë°ì´í„° í™•ì¸
            existing_count = await self.pattern_collection.count_documents({})
            logger.info(f"ê¸°ì¡´ context_patterns ë¬¸ì„œ ìˆ˜: {existing_count}")
            
            # ìƒˆë¡œìš´ íŒ¨í„´ ë°ì´í„° ìƒì„±
            patterns_to_insert = []
            for keyword in keywords:
                pattern_data = {
                    'pattern': keyword,
                    'context': 'technical',
                    'priority': 1,
                    'description': f'LLM ì¶”ì¶œ technical í‚¤ì›Œë“œ: {keyword}',
                    'created_at': datetime.utcnow(),
                    'updated_at': datetime.utcnow(),
                    'source': 'llm_extraction'
                }
                patterns_to_insert.append(pattern_data)
            
            # ì¼ê´„ ì‚½ì…
            if patterns_to_insert:
                result = await self.pattern_collection.insert_many(patterns_to_insert)
                inserted_count = len(result.inserted_ids)
                logger.info(f"ì €ì¥ ì™„ë£Œ: {inserted_count}ê°œì˜ íŒ¨í„´ì´ ì €ì¥ë¨")
                
                # ìµœì¢… í™•ì¸
                final_count = await self.pattern_collection.count_documents({})
                logger.info(f"ìµœì¢… context_patterns ë¬¸ì„œ ìˆ˜: {final_count}")
                
                return True
            else:
                logger.warning("ì €ì¥í•  íŒ¨í„´ì´ ì—†ìŠµë‹ˆë‹¤.")
                return False
                
        except Exception as e:
            logger.error(f"context_patterns ì €ì¥ ì‹¤íŒ¨: {e}")
            return False
    
    async def run_full_process(self):
        """ì „ì²´ í”„ë¡œì„¸ìŠ¤ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        logger.info("=== ì „ì²´ Technical í‚¤ì›Œë“œ ì¶”ì¶œ í”„ë¡œì„¸ìŠ¤ ì‹œì‘ ===")
        
        try:
            # 1ë‹¨ê³„: ëª¨ë“  ì§ˆë¬¸ ê°€ì ¸ì˜¤ê¸°
            questions = await self.get_all_questions()
            if not questions:
                logger.error("ì§ˆë¬¸ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return False
            
            # 2ë‹¨ê³„: LLMìœ¼ë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ
            results = await self.process_all_questions(questions)
            if not results:
                logger.error("í‚¤ì›Œë“œ ì¶”ì¶œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                return False
            
            # 3ë‹¨ê³„: ì¤‘ë³µ ì œê±°
            unique_keywords = self.remove_duplicates(results)
            if not unique_keywords:
                logger.error("ê³ ìœ í•œ í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
                return False
            
            # 4ë‹¨ê³„: context_patternsì— ì €ì¥
            success = await self.save_to_context_patterns(unique_keywords)
            
            if success:
                logger.info("âœ… ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ!")
                logger.info(f"ì´ ì²˜ë¦¬ëœ ì§ˆë¬¸ ìˆ˜: {len(questions)}")
                logger.info(f"í‚¤ì›Œë“œ ì¶”ì¶œëœ ì§ˆë¬¸ ìˆ˜: {len(results)}")
                logger.info(f"ìµœì¢… ê³ ìœ  í‚¤ì›Œë“œ ìˆ˜: {len(unique_keywords)}")
                return True
            else:
                logger.error("âŒ context_patterns ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                return False
                
        except Exception as e:
            logger.error(f"ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return False

async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    try:
        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
        db = await get_database()
        logger.info("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì™„ë£Œ")
        
        # LLM ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        llm_service = LLMService()
        logger.info("LLM ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
        
        # ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰ê¸° ìƒì„±
        extractor = FullTechnicalKeywordExtractor(db, llm_service)
        
        # ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰
        success = await extractor.run_full_process()
        
        if success:
            logger.info("ğŸ‰ ì „ì²´ ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        else:
            logger.error("âŒ ì „ì²´ ì‘ì—…ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            
    except Exception as e:
        logger.error(f"ë©”ì¸ í•¨ìˆ˜ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    asyncio.run(main()) 