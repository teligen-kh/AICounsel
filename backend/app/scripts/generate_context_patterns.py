"""
knowledge_base â†’ context_patterns ìë™ ë³€í™˜ ìŠ¤í¬ë¦½íŠ¸
LLMì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì„ ë¬¸ë§¥ë³„ë¡œ ë¶„ë¥˜í•˜ê³  ë‹¤ì–‘í•œ íŒ¨í„´ ìƒì„±
"""

import asyncio
import motor.motor_asyncio
from datetime import datetime
import logging
from typing import List, Dict, Set
import re
import random

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ContextPatternGenerator:
    """knowledge_base â†’ context_patterns ìë™ ë³€í™˜"""
    
    def __init__(self, db):
        self.db = db
        self.knowledge_collection = db.knowledge_base
        self.pattern_collection = db.context_patterns
        
        # ë¬¸ë§¥ë³„ í‚¤ì›Œë“œ ë§¤í•‘
        self.context_keywords = {
            'technical': [
                'í”„ë¦°í„°', 'í¬ìŠ¤', 'pos', 'ì„¤ì¹˜', 'ì„¤ì •', 'ì˜¤ë¥˜', 'ì—ëŸ¬', 'ë¬¸ì œ', 'í•´ê²°',
                'í”„ë¡œê·¸ë¨', 'ì†Œí”„íŠ¸ì›¨ì–´', 'í•˜ë“œì›¨ì–´', 'ê¸°ê¸°', 'ì¥ë¹„', 'ìŠ¤ìºë„ˆ',
                'ì‹œìŠ¤í…œ', 'ë„¤íŠ¸ì›Œí¬', 'ì—°ê²°', 'ì¸í„°ë„·', 'ë°ì´í„°', 'ë°±ì—…', 'ë³µêµ¬',
                'ì—…ë°ì´íŠ¸', 'ë‹¨ë§ê¸°', 'ì¹´ë“œ', 'ê²°ì œ', 'ì˜ìˆ˜ì¦', 'ë°”ì½”ë“œ', 'qrì½”ë“œ',
                'ë“œë¼ì´ë²„', 'ì¬ì„¤ì¹˜', 'í‚¤ì˜¤ìŠ¤í¬', 'kiosk', 'ì¹´ë“œë¦¬ë”ê¸°'
            ],
            'casual': [
                'ì•ˆë…•', 'ë°˜ê°‘', 'í•˜ì´', 'hello', 'hi', 'ë°”ì˜', 'ì‹ì‚¬', 'ì ì‹¬', 'ì €ë…',
                'ì»¤í”¼', 'ì°¨', 'ë‚ ì”¨', 'ê¸°ë¶„', 'í”¼ê³¤', 'í˜ë“œ', 'ì–´ë–»ê²Œ ì§€ë‚´', 'ì˜ ì§€ë‚´',
                'ë„ˆëŠ”', 'ë‹¹ì‹ ì€', 'ai', 'ì¸ê³µì§€ëŠ¥', 'ë¡œë´‡'
            ],
            'non_counseling': [
                'í•œêµ­', 'ëŒ€í•œë¯¼êµ­', 'ì¼ë³¸', 'ì¤‘êµ­', 'ë¯¸êµ­', 'ì—­ì‚¬', 'ì§€ë¦¬', 'ìˆ˜ë„',
                'ì¸êµ¬', 'ë©´ì ', 'ì–¸ì–´', 'ë¬¸í™”', 'ì •ì¹˜', 'ê²½ì œ', 'ì‚¬íšŒ', 'ê³¼í•™',
                'ìˆ˜í•™', 'ë¬¼ë¦¬', 'í™”í•™', 'ìƒë¬¼'
            ],
            'profanity': [
                'ë°”ë³´', 'ë©ì²­', 'ë˜¥', 'ê°œ', 'ìƒˆë¼', 'ì”¨ë°œ', 'ë³‘ì‹ ', 'ë¯¸ì¹œ', 'ë¯¸ì³¤',
                'ëŒì•˜', 'ì •ì‹ ', 'ë¹¡', 'ë¹¡ì¹˜'
            ]
        }
        
        # LLM ì„œë¹„ìŠ¤ (ë‚˜ì¤‘ì— ì£¼ì…)
        self.llm_service = None
    
    def inject_llm_service(self, llm_service):
        """LLM ì„œë¹„ìŠ¤ ì£¼ì…"""
        self.llm_service = llm_service
    
    def _classify_by_keywords(self, question: str) -> str:
        """í‚¤ì›Œë“œ ê¸°ë°˜ ë¬¸ë§¥ ë¶„ë¥˜"""
        question_lower = question.lower()
        
        # ê° ë¬¸ë§¥ë³„ í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
        scores = {}
        for context, keywords in self.context_keywords.items():
            score = sum(1 for keyword in keywords if keyword in question_lower)
            scores[context] = score
        
        # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ ë¬¸ë§¥ ë°˜í™˜
        if scores:
            return max(scores, key=scores.get)
        
        return 'technical'  # ê¸°ë³¸ê°’
    
    def _generate_variations(self, question: str, context: str) -> List[str]:
        """ì§ˆë¬¸ì˜ ë‹¤ì–‘í•œ ë³€í˜• ìƒì„±"""
        variations = [question]  # ì›ë³¸ í¬í•¨
        
        # ê¸°ë³¸ ë³€í˜• íŒ¨í„´
        basic_patterns = [
            # ì§ˆë¬¸ í˜•íƒœ ë³€í˜•
            lambda q: q.replace('?', '').strip(),
            lambda q: q.replace('?', 'ìš”').strip(),
            lambda q: q.replace('?', 'ì¸ê°€ìš”').strip(),
            lambda q: q.replace('?', 'í• ê¹Œìš”').strip(),
            
            # ì¡´ëŒ“ë§ ë³€í˜•
            lambda q: q.replace('ìš”', '').strip(),
            lambda q: q.replace('ìš”', 'ë‹¤').strip(),
            
            # ì¡°ì‚¬ ë³€í˜•
            lambda q: q.replace('ê°€', 'ì´').strip(),
            lambda q: q.replace('ì´', 'ê°€').strip(),
            lambda q: q.replace('ë¥¼', 'ì„').strip(),
            lambda q: q.replace('ì„', 'ë¥¼').strip(),
        ]
        
        # ê¸°ë³¸ ë³€í˜• ì ìš©
        for pattern in basic_patterns:
            try:
                variation = pattern(question)
                if variation != question and len(variation) > 5:
                    variations.append(variation)
            except:
                continue
        
        # ë¬¸ë§¥ë³„ íŠ¹í™” ë³€í˜•
        if context == 'technical':
            tech_variations = [
                lambda q: f"{q} ì–´ë–»ê²Œ í•´ê²°í•˜ë‚˜ìš”?",
                lambda q: f"{q} ë°©ë²• ì•Œë ¤ì£¼ì„¸ìš”",
                lambda q: f"{q} ì„¤ì • ë°©ë²•",
                lambda q: f"{q} ë¬¸ì œ í•´ê²°",
                lambda q: f"{q} ì˜¤ë¥˜ ìˆ˜ì •",
            ]
            
            for pattern in tech_variations:
                try:
                    variation = pattern(question)
                    if len(variation) < 100:  # ë„ˆë¬´ ê¸¸ì§€ ì•Šê²Œ
                        variations.append(variation)
                except:
                    continue
        
        # ì¤‘ë³µ ì œê±° ë° ê¸¸ì´ ì œí•œ
        unique_variations = []
        seen = set()
        for variation in variations:
            if variation not in seen and 5 <= len(variation) <= 100:
                unique_variations.append(variation)
                seen.add(variation)
        
        return unique_variations[:10]  # ìµœëŒ€ 10ê°œ ë³€í˜•
    
    async def _classify_with_llm(self, question: str) -> str:
        """LLMì„ ì‚¬ìš©í•œ ë¬¸ë§¥ ë¶„ë¥˜"""
        if not self.llm_service:
            return self._classify_by_keywords(question)
        
        try:
            prompt = f"""ë‹¤ìŒ ì§ˆë¬¸ì„ 4ê°€ì§€ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜í•´ì£¼ì„¸ìš”:

- casual: ì¼ìƒ ëŒ€í™”, ì¸ì‚¬ë§, AI ê´€ë ¨ ì§ˆë¬¸
- technical: ê¸°ìˆ ì  ë¬¸ì œ í•´ê²°, ì‹œìŠ¤í…œ ê´€ë ¨ ì§ˆë¬¸, í•˜ë“œì›¨ì–´/ì†Œí”„íŠ¸ì›¨ì–´ ë¬¸ì œ
- non_counseling: ìƒë‹´ ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ëŠ” ì¼ë°˜ ì§€ì‹ ì§ˆë¬¸, ì—­ì‚¬/ì§€ë¦¬/ê³¼í•™ ë“±
- profanity: ìš•ì„¤ ë° ê³µê²©ì  í‘œí˜„

ì§ˆë¬¸: "{question}"

ë‹µë³€ í˜•ì‹: casual ë˜ëŠ” technical ë˜ëŠ” non_counseling ë˜ëŠ” profanity
ë‹µë³€ë§Œ ì¶œë ¥í•˜ê³  ë‹¤ë¥¸ ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”."""

            response = await self.llm_service.get_conversation_response(prompt)
            response_clean = response.strip().lower()
            
            # ì‘ë‹µ íŒŒì‹±
            if 'casual' in response_clean:
                return 'casual'
            elif 'technical' in response_clean:
                return 'technical'
            elif 'non_counseling' in response_clean:
                return 'non_counseling'
            elif 'profanity' in response_clean:
                return 'profanity'
            else:
                return self._classify_by_keywords(question)
                
        except Exception as e:
            logging.error(f"LLM ë¶„ë¥˜ ì‹¤íŒ¨: {str(e)}")
            return self._classify_by_keywords(question)
    
    async def generate_patterns_from_knowledge_base(self, use_llm: bool = True) -> Dict:
        """knowledge_baseì—ì„œ context_patterns ìƒì„±"""
        logger.info("knowledge_base â†’ context_patterns ë³€í™˜ ì‹œì‘...")
        
        # ê¸°ì¡´ íŒ¨í„´ ìˆ˜ í™•ì¸
        existing_count = await self.pattern_collection.count_documents({})
        logger.info(f"ê¸°ì¡´ context_patterns ìˆ˜: {existing_count}")
        
        # knowledge_baseì—ì„œ ëª¨ë“  ì§ˆë¬¸ ê°€ì ¸ì˜¤ê¸°
        questions = []
        async for doc in self.knowledge_collection.find({}):
            question = doc.get("question", "").strip()
            if question and len(question) > 5:
                questions.append({
                    "question": question,
                    "answer": doc.get("answer", ""),
                    "category": doc.get("category", "technical"),
                    "keywords": doc.get("keywords", [])
                })
        
        logger.info(f"ì²˜ë¦¬í•  ì§ˆë¬¸ ìˆ˜: {len(questions)}")
        
        # ë¬¸ë§¥ë³„ í†µê³„
        context_stats = {'technical': 0, 'casual': 0, 'non_counseling': 0, 'profanity': 0}
        total_patterns = 0
        
        # ê° ì§ˆë¬¸ì— ëŒ€í•´ íŒ¨í„´ ìƒì„±
        for i, q_data in enumerate(questions, 1):
            question = q_data["question"]
            
            # ë¬¸ë§¥ ë¶„ë¥˜
            if use_llm and self.llm_service:
                context = await self._classify_with_llm(question)
            else:
                context = self._classify_by_keywords(question)
            
            context_stats[context] += 1
            
            # ë³€í˜• íŒ¨í„´ ìƒì„±
            variations = self._generate_variations(question, context)
            
            # íŒ¨í„´ ì €ì¥
            for variation in variations:
                pattern_doc = {
                    "pattern": variation,
                    "context": context,
                    "original_question": question,
                    "is_active": True,
                    "accuracy": 0.9,
                    "usage_count": 0,
                    "created_at": datetime.now(),
                    "updated_at": datetime.now(),
                    "source": "knowledge_base_generated"
                }
                
                # ì¤‘ë³µ ì²´í¬ í›„ ì €ì¥
                existing = await self.pattern_collection.find_one({"pattern": variation})
                if not existing:
                    await self.pattern_collection.insert_one(pattern_doc)
                    total_patterns += 1
            
            # ì§„í–‰ë¥  ì¶œë ¥
            if i % 50 == 0:
                logger.info(f"ì§„í–‰ë¥ : {i}/{len(questions)} ({i/len(questions)*100:.1f}%)")
        
        # ìµœì¢… í†µê³„
        final_count = await self.pattern_collection.count_documents({})
        
        result = {
            "original_patterns": existing_count,
            "new_patterns_generated": total_patterns,
            "total_patterns": final_count,
            "context_distribution": context_stats,
            "questions_processed": len(questions)
        }
        
        logger.info("=== ë³€í™˜ ì™„ë£Œ ===")
        logger.info(f"ê¸°ì¡´ íŒ¨í„´: {existing_count}ê°œ")
        logger.info(f"ìƒˆë¡œ ìƒì„±: {total_patterns}ê°œ")
        logger.info(f"ì´ íŒ¨í„´: {final_count}ê°œ")
        logger.info("ë¬¸ë§¥ë³„ ë¶„í¬:")
        for context, count in context_stats.items():
            logger.info(f"  - {context}: {count}ê°œ")
        
        return result
    
    async def cleanup_duplicate_patterns(self) -> Dict:
        """ì¤‘ë³µ íŒ¨í„´ ì •ë¦¬"""
        logger.info("ì¤‘ë³µ íŒ¨í„´ ì •ë¦¬ ì‹œì‘...")
        
        # ëª¨ë“  íŒ¨í„´ ê°€ì ¸ì˜¤ê¸°
        patterns = []
        async for doc in self.pattern_collection.find({}):
            patterns.append(doc)
        
        # ì¤‘ë³µ íŒ¨í„´ ì°¾ê¸°
        seen_patterns = set()
        duplicates = []
        
        for pattern_doc in patterns:
            pattern = pattern_doc["pattern"].strip().lower()
            if pattern in seen_patterns:
                duplicates.append(pattern_doc["_id"])
            else:
                seen_patterns.add(pattern)
        
        # ì¤‘ë³µ íŒ¨í„´ ì‚­ì œ
        if duplicates:
            result = await self.pattern_collection.delete_many({"_id": {"$in": duplicates}})
            deleted_count = result.deleted_count
        else:
            deleted_count = 0
        
        final_count = await self.pattern_collection.count_documents({})
        
        logger.info(f"ì¤‘ë³µ íŒ¨í„´ ì‚­ì œ: {deleted_count}ê°œ")
        logger.info(f"ìµœì¢… íŒ¨í„´ ìˆ˜: {final_count}ê°œ")
        
        return {
            "duplicates_removed": deleted_count,
            "final_patterns": final_count
        }

async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    # MongoDB ì—°ê²°
    client = motor.motor_asyncio.AsyncIOMotorClient("mongodb://localhost:27017")
    db = client.aicounsel
    
    try:
        generator = ContextPatternGenerator(db)
        
        # LLM ì„œë¹„ìŠ¤ ì£¼ì… (ì„ íƒì‚¬í•­)
        try:
            from ..dependencies import get_llm_service
            llm_service = await get_llm_service()
            generator.inject_llm_service(llm_service)
            use_llm = True
            print("âœ… LLM ì„œë¹„ìŠ¤ ì—°ê²°ë¨ - ê³ ì •í™•ë„ ë¶„ë¥˜ ì‚¬ìš©")
        except:
            use_llm = False
            print("âš ï¸ LLM ì„œë¹„ìŠ¤ ì—°ê²° ì‹¤íŒ¨ - í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ë¥˜ ì‚¬ìš©")
        
        # 1. íŒ¨í„´ ìƒì„±
        result = await generator.generate_patterns_from_knowledge_base(use_llm=use_llm)
        
        # 2. ì¤‘ë³µ ì •ë¦¬
        cleanup_result = await generator.cleanup_duplicate_patterns()
        
        # 3. ìµœì¢… ê²°ê³¼ ì¶œë ¥
        print(f"\nğŸ‰ context_patterns ìƒì„± ì™„ë£Œ!")
        print(f"   - ê¸°ì¡´: {result['original_patterns']}ê°œ")
        print(f"   - ìƒˆë¡œ ìƒì„±: {result['new_patterns_generated']}ê°œ")
        print(f"   - ì¤‘ë³µ ì œê±°: {cleanup_result['duplicates_removed']}ê°œ")
        print(f"   - ìµœì¢…: {cleanup_result['final_patterns']}ê°œ")
        print(f"   - ì²˜ë¦¬ëœ ì§ˆë¬¸: {result['questions_processed']}ê°œ")
        
        print(f"\nğŸ“Š ë¬¸ë§¥ë³„ ë¶„í¬:")
        for context, count in result['context_distribution'].items():
            print(f"   - {context}: {count}ê°œ")
        
    except Exception as e:
        logger.error(f"ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
    finally:
        client.close()

if __name__ == "__main__":
    asyncio.run(main()) 