import os
import sys
import logging
from llama_cpp import Llama

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def test_phi35_model():
    """Phi-3.5 ëª¨ë¸ ê¸°ë³¸ í…ŒìŠ¤íŠ¸"""
    
    # ëª¨ë¸ ê²½ë¡œ ì„¤ì •
    base_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), "models")
    model_path = os.path.join(base_path, "Phi-3.5-mini-instruct-Q8_0.gguf")
    
    print(f"ëª¨ë¸ ê²½ë¡œ: {model_path}")
    print(f"ëª¨ë¸ íŒŒì¼ ì¡´ì¬: {os.path.exists(model_path)}")
    
    if not os.path.exists(model_path):
        print("âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        return False
    
    try:
        print("ğŸ”„ Phi-3.5 ëª¨ë¸ ë¡œë”© ì¤‘...")
        
        # llama-cpp ëª¨ë¸ ì´ˆê¸°í™”
        llm = Llama(
            model_path=model_path,
            n_ctx=2048,           # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´
            n_threads=8,          # CPU ìŠ¤ë ˆë“œ ìˆ˜
            n_gpu_layers=0,       # CPU ëª¨ë“œ
            verbose=False,        # ë¡œê·¸ ìµœì†Œí™”
            seed=42              # ì¬í˜„ ê°€ëŠ¥ì„±
        )
        
        print("âœ… Phi-3.5 ëª¨ë¸ ë¡œë”© ì„±ê³µ!")
        
        # í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë“¤
        test_prompts = [
            "ì•ˆë…•í•˜ì„¸ìš”",
            "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì–´ë•Œìš”?",
            "í•œêµ­ì–´ë¡œ ëŒ€í™”í•´ì£¼ì„¸ìš”",
            "ê°„ë‹¨í•œ ì¸ì‚¬ë§ì„ í•´ì£¼ì„¸ìš”"
        ]
        
        print("\n=== Phi-3.5 ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")
        
        for i, prompt in enumerate(test_prompts, 1):
            print(f"\n--- í…ŒìŠ¤íŠ¸ {i}: {prompt} ---")
            
            try:
                # Phi-3.5 ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ í˜•ì‹ ì‚¬ìš©
                formatted_prompt = f"<|user|>\n{prompt}<|end|>\n<|assistant|>\n"
                
                print(f"í”„ë¡¬í”„íŠ¸: {formatted_prompt}")
                
                # ì‘ë‹µ ìƒì„±
                response = llm(
                    formatted_prompt,
                    max_tokens=100,
                    temperature=0.7,
                    top_p=0.9,
                    top_k=50,
                    repeat_penalty=1.1,
                    stop=["<|user|>", "<|end|>"]
                )
                
                generated_text = response.get('choices', [{}])[0].get('text', '').strip()
                print(f"ì‘ë‹µ: {generated_text}")
                
            except Exception as e:
                print(f"âŒ í…ŒìŠ¤íŠ¸ {i} ì‹¤íŒ¨: {str(e)}")
        
        print("\n=== í…ŒìŠ¤íŠ¸ ì™„ë£Œ ===")
        return True
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {str(e)}")
        return False

if __name__ == "__main__":
    success = test_phi35_model()
    if success:
        print("\nğŸ‰ Phi-3.5 ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
    else:
        print("\nğŸ’¥ Phi-3.5 ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨!") 