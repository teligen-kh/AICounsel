#!/usr/bin/env python3
"""
ìµœì í™”ëœ íŒ¨í„´ ë§¤ì¹­ ì„œë¹„ìŠ¤
MongoDB Text Search + ë²¡í„° ê²€ìƒ‰ ì¡°í•©ìœ¼ë¡œ ì†ë„ì™€ ì •í™•ë„ ìµœì í™”
"""

import logging
import re
from typing import Dict, List, Optional, Tuple, Set
from motor.motor_asyncio import AsyncIOMotorDatabase
from datetime import datetime
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import asyncio

logger = logging.getLogger(__name__)

class OptimizedPatternMatcher:
    """ìµœì í™”ëœ íŒ¨í„´ ë§¤ì¹­ê¸°"""
    
    def __init__(self, db: AsyncIOMotorDatabase):
        self.db = db
        self.pattern_collection = db.context_patterns
        self.cache = {}  # ê°„ë‹¨í•œ ë©”ëª¨ë¦¬ ìºì‹œ
        self.cache_ttl = 300  # 5ë¶„ ìºì‹œ TTL
        self.last_cache_update = 0
        
        # TF-IDF ë²¡í„°ë¼ì´ì € (í•œêµ­ì–´ ìµœì í™”)
        self.vectorizer = TfidfVectorizer(
            analyzer='word',
            ngram_range=(1, 3),  # 1-3ê·¸ë¨ìœ¼ë¡œ ë‹¨ì–´ ì¡°í•© ìºì¹˜
            min_df=1,
            max_df=0.9,
            stop_words=None,  # í•œêµ­ì–´ëŠ” ë¶ˆìš©ì–´ ì²˜ë¦¬ê°€ ë³µì¡í•˜ë¯€ë¡œ ì œì™¸
            lowercase=True
        )
        
        # íŒ¨í„´ ë²¡í„° ìºì‹œ
        self.pattern_vectors = None
        self.pattern_texts = []
        self.pattern_docs = []
        
        # ì„±ëŠ¥ í†µê³„
        self.stats = {
            'text_search_count': 0,
            'vector_search_count': 0,
            'cache_hits': 0,
            'total_queries': 0,
            'avg_response_time': 0
        }
    
    async def initialize(self):
        """íŒ¨í„´ ë§¤ì¹­ê¸° ì´ˆê¸°í™”"""
        try:
            logger.info("ğŸ”§ ìµœì í™”ëœ íŒ¨í„´ ë§¤ì¹­ê¸° ì´ˆê¸°í™” ì‹œì‘")
            
            # 1. MongoDB Text Index ìƒì„± (í•œ ë²ˆë§Œ)
            await self._ensure_text_index()
            
            # 2. íŒ¨í„´ ë°ì´í„° ë¡œë“œ ë° ë²¡í„°í™”
            await self._load_and_vectorize_patterns()
            
            logger.info(f"âœ… íŒ¨í„´ ë§¤ì¹­ê¸° ì´ˆê¸°í™” ì™„ë£Œ - {len(self.pattern_docs)}ê°œ íŒ¨í„´ ë¡œë“œ")
            
        except Exception as e:
            logger.error(f"íŒ¨í„´ ë§¤ì¹­ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    async def _ensure_text_index(self):
        """MongoDB Text Index ìƒì„±"""
        try:
            # ê¸°ì¡´ ì¸ë±ìŠ¤ í™•ì¸
            indexes = await self.pattern_collection.list_indexes().to_list()
            has_text_index = any(idx.get('name') == 'pattern_text' for idx in indexes)
            
            if not has_text_index:
                # Text Index ìƒì„± (í•œêµ­ì–´ ì§€ì›)
                await self.pattern_collection.create_index([
                    ("pattern", "text")
                ], name="pattern_text", default_language="none")
                
                logger.info("âœ… MongoDB Text Index ìƒì„± ì™„ë£Œ")
            else:
                logger.info("âœ… MongoDB Text Index ì´ë¯¸ ì¡´ì¬")
                
        except Exception as e:
            logger.error(f"Text Index ìƒì„± ì‹¤íŒ¨: {e}")
    
    async def _load_and_vectorize_patterns(self):
        """íŒ¨í„´ ë°ì´í„° ë¡œë“œ ë° ë²¡í„°í™”"""
        try:
            # ëª¨ë“  íŒ¨í„´ ë¡œë“œ
            cursor = self.pattern_collection.find({})
            self.pattern_docs = await cursor.to_list(length=None)
            
            if not self.pattern_docs:
                logger.warning("âš ï¸ ë¡œë“œí•  íŒ¨í„´ì´ ì—†ìŠµë‹ˆë‹¤")
                return
            
            # íŒ¨í„´ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            self.pattern_texts = [doc['pattern'] for doc in self.pattern_docs]
            
            # TF-IDF ë²¡í„°í™”
            self.pattern_vectors = self.vectorizer.fit_transform(self.pattern_texts)
            
            logger.info(f"âœ… {len(self.pattern_docs)}ê°œ íŒ¨í„´ ë²¡í„°í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"íŒ¨í„´ ë¡œë“œ ë° ë²¡í„°í™” ì‹¤íŒ¨: {e}")
            raise
    
    async def find_best_match(self, user_input: str, threshold: float = 0.3) -> Optional[Tuple[Dict, float, str]]:
        """
        ì‚¬ìš©ì ì…ë ¥ì— ëŒ€í•œ ìµœì  íŒ¨í„´ ë§¤ì¹­
        
        Returns:
            Tuple[pattern_doc, similarity_score, method_used] or None
        """
        start_time = datetime.now()
        
        try:
            self.stats['total_queries'] += 1
            
            # 1. ìºì‹œ ì²´í¬
            cache_key = user_input.lower().strip()
            if cache_key in self.cache:
                cache_entry = self.cache[cache_key]
                if (datetime.now() - cache_entry['timestamp']).seconds < self.cache_ttl:
                    self.stats['cache_hits'] += 1
                    return cache_entry['result']
            
            # 2. 1ë‹¨ê³„: MongoDB Text Search (ë¹ ë¥¸ í•„í„°ë§)
            text_candidates = await self._text_search(user_input)
            
            if not text_candidates:
                # Text Search ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ í™•ì¥
                vector_result = await self._vector_search(user_input, threshold)
                if vector_result:
                    pattern_doc, score, method = vector_result
                    result = (pattern_doc, score, method)
                    self._update_cache(cache_key, result)
                    return result
                return None
            
            # 3. 2ë‹¨ê³„: Text Search í›„ë³´ë“¤ì— ëŒ€í•´ ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ ì •í™•ë„ í–¥ìƒ
            best_match = None
            best_score = 0
            best_method = "text_search"
            
            for pattern_doc in text_candidates:
                vector_score = await self._calculate_similarity(user_input, pattern_doc['pattern'])
                
                # Text Search ê²°ê³¼ì— ê°€ì¤‘ì¹˜ ë¶€ì—¬ (ë¹ ë¥¸ ë§¤ì¹­ ìš°ì„ )
                weighted_score = vector_score * 1.2 if vector_score > threshold else vector_score
                
                if weighted_score > best_score:
                    best_score = weighted_score
                    best_match = pattern_doc
                    best_method = "hybrid"
            
            # 4. ê²°ê³¼ ë°˜í™˜
            if best_match and best_score >= threshold:
                result = (best_match, best_score, best_method)
                self._update_cache(cache_key, result)
                
                # ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸
                response_time = (datetime.now() - start_time).total_seconds() * 1000
                self._update_stats(response_time)
                
                return result
            
            return None
            
        except Exception as e:
            logger.error(f"íŒ¨í„´ ë§¤ì¹­ ì¤‘ ì˜¤ë¥˜: {e}")
            return None
    
    async def _text_search(self, user_input: str) -> List[Dict]:
        """MongoDB Text Search ìˆ˜í–‰"""
        try:
            self.stats['text_search_count'] += 1
            
            # Text Search ì¿¼ë¦¬ (í•œêµ­ì–´ ìµœì í™”)
            query = {
                "$text": {
                    "$search": user_input,
                    "$language": "none"  # í•œêµ­ì–´ëŠ” ì–¸ì–´ ê°ì§€ ë¹„í™œì„±í™”
                }
            }
            
            # Text Scoreë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ ê²°ê³¼ë§Œ ë°˜í™˜
            cursor = self.pattern_collection.find(query).sort([
                ("score", {"$meta": "textScore"})
            ]).limit(10)  # ìƒìœ„ 10ê°œë§Œ
            
            results = await cursor.to_list(length=10)
            
            # Text Scoreê°€ 1.0 ì´ìƒì¸ ê²°ê³¼ë§Œ ë°˜í™˜ (ì˜ë¯¸ìˆëŠ” ë§¤ì¹­)
            filtered_results = [
                doc for doc in results 
                if doc.get('score', 0) >= 1.0
            ]
            
            logger.debug(f"Text Search ê²°ê³¼: {len(filtered_results)}ê°œ")
            return filtered_results
            
        except Exception as e:
            logger.error(f"Text Search ì‹¤íŒ¨: {e}")
            return []
    
    async def _vector_search(self, user_input: str, threshold: float) -> Optional[Tuple[Dict, float, str]]:
        """ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰"""
        try:
            self.stats['vector_search_count'] += 1
            
            if not self.pattern_vectors or not self.pattern_texts:
                return None
            
            # ì‚¬ìš©ì ì…ë ¥ ë²¡í„°í™”
            user_vector = self.vectorizer.transform([user_input])
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            similarities = cosine_similarity(user_vector, self.pattern_vectors).flatten()
            
            # ìµœê³  ìœ ì‚¬ë„ íŒ¨í„´ ì°¾ê¸°
            best_idx = np.argmax(similarities)
            best_score = similarities[best_idx]
            
            if best_score >= threshold:
                pattern_doc = self.pattern_docs[best_idx]
                logger.debug(f"ë²¡í„° ê²€ìƒ‰ ê²°ê³¼: {pattern_doc['pattern']} (ìœ ì‚¬ë„: {best_score:.3f})")
                return pattern_doc, best_score, "vector_search"
            
            return None
            
        except Exception as e:
            logger.error(f"ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return None
    
    async def _calculate_similarity(self, user_input: str, pattern: str) -> float:
        """ë‘ í…ìŠ¤íŠ¸ ê°„ì˜ ìœ ì‚¬ë„ ê³„ì‚°"""
        try:
            if not self.pattern_vectors or not self.pattern_texts:
                return 0.0
            
            # ë²¡í„°í™”
            user_vector = self.vectorizer.transform([user_input])
            pattern_vector = self.vectorizer.transform([pattern])
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„
            similarity = cosine_similarity(user_vector, pattern_vector)[0][0]
            return float(similarity)
            
        except Exception as e:
            logger.error(f"ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0
    
    def _update_cache(self, key: str, result: Tuple):
        """ìºì‹œ ì—…ë°ì´íŠ¸"""
        self.cache[key] = {
            'result': result,
            'timestamp': datetime.now()
        }
        
        # ìºì‹œ í¬ê¸° ì œí•œ (ë©”ëª¨ë¦¬ ê´€ë¦¬)
        if len(self.cache) > 1000:
            # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±°
            oldest_key = min(self.cache.keys(), 
                           key=lambda k: self.cache[k]['timestamp'])
            del self.cache[oldest_key]
    
    def _update_stats(self, response_time: float):
        """ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸"""
        current_avg = self.stats['avg_response_time']
        total_queries = self.stats['total_queries']
        
        # ì´ë™ í‰ê·  ê³„ì‚°
        self.stats['avg_response_time'] = (
            (current_avg * (total_queries - 1) + response_time) / total_queries
        )
    
    async def refresh_patterns(self):
        """íŒ¨í„´ ë°ì´í„° ìƒˆë¡œê³ ì¹¨"""
        try:
            logger.info("ğŸ”„ íŒ¨í„´ ë°ì´í„° ìƒˆë¡œê³ ì¹¨ ì‹œì‘")
            
            # ìºì‹œ í´ë¦¬ì–´
            self.cache.clear()
            self.last_cache_update = datetime.now().timestamp()
            
            # íŒ¨í„´ ì¬ë¡œë“œ ë° ë²¡í„°í™”
            await self._load_and_vectorize_patterns()
            
            logger.info("âœ… íŒ¨í„´ ë°ì´í„° ìƒˆë¡œê³ ì¹¨ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"íŒ¨í„´ ìƒˆë¡œê³ ì¹¨ ì‹¤íŒ¨: {e}")
    
    def get_performance_stats(self) -> Dict:
        """ì„±ëŠ¥ í†µê³„ ë°˜í™˜"""
        return {
            **self.stats,
            'cache_size': len(self.cache),
            'pattern_count': len(self.pattern_docs),
            'cache_hit_rate': (
                self.stats['cache_hits'] / max(self.stats['total_queries'], 1)
            )
        }
    
    async def add_pattern(self, pattern: str, context: str, description: str = ""):
        """ìƒˆ íŒ¨í„´ ì¶”ê°€"""
        try:
            # MongoDBì— íŒ¨í„´ ì¶”ê°€
            pattern_doc = {
                "pattern": pattern,
                "context": context,
                "description": description,
                "created_at": datetime.utcnow(),
                "usage_count": 0,
                "accuracy": 0.9
            }
            
            await self.pattern_collection.insert_one(pattern_doc)
            
            # íŒ¨í„´ ë°ì´í„° ìƒˆë¡œê³ ì¹¨
            await self.refresh_patterns()
            
            logger.info(f"âœ… ìƒˆ íŒ¨í„´ ì¶”ê°€: {pattern}")
            
        except Exception as e:
            logger.error(f"íŒ¨í„´ ì¶”ê°€ ì‹¤íŒ¨: {e}")
    
    async def optimize_patterns(self):
        """íŒ¨í„´ ìµœì í™” (ì‚¬ìš© ë¹ˆë„ ê¸°ë°˜)"""
        try:
            logger.info("ğŸ”§ íŒ¨í„´ ìµœì í™” ì‹œì‘")
            
            # ì‚¬ìš© ë¹ˆë„ê°€ ë‚®ì€ íŒ¨í„´ë“¤ ì°¾ê¸°
            low_usage_patterns = await self.pattern_collection.find({
                "usage_count": {"$lt": 5}
            }).to_list(length=None)
            
            logger.info(f"ì‚¬ìš© ë¹ˆë„ ë‚®ì€ íŒ¨í„´: {len(low_usage_patterns)}ê°œ")
            
            # ì •í™•ë„ê°€ ë‚®ì€ íŒ¨í„´ë“¤ ì°¾ê¸°
            low_accuracy_patterns = await self.pattern_collection.find({
                "accuracy": {"$lt": 0.7}
            }).to_list(length=None)
            
            logger.info(f"ì •í™•ë„ ë‚®ì€ íŒ¨í„´: {len(low_accuracy_patterns)}ê°œ")
            
            # ìµœì í™” ì œì•ˆ
            optimization_suggestions = []
            
            for pattern in low_usage_patterns:
                optimization_suggestions.append({
                    "pattern_id": str(pattern["_id"]),
                    "pattern": pattern["pattern"],
                    "issue": "low_usage",
                    "suggestion": "íŒ¨í„´ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ ìˆ˜ì •í•˜ê±°ë‚˜ ì œê±° ê³ ë ¤"
                })
            
            for pattern in low_accuracy_patterns:
                optimization_suggestions.append({
                    "pattern_id": str(pattern["_id"]),
                    "pattern": pattern["pattern"],
                    "issue": "low_accuracy",
                    "suggestion": "íŒ¨í„´ì„ ë” ì •í™•í•˜ê²Œ ìˆ˜ì •"
                })
            
            logger.info(f"âœ… íŒ¨í„´ ìµœì í™” ì™„ë£Œ - {len(optimization_suggestions)}ê°œ ì œì•ˆ")
            return optimization_suggestions
            
        except Exception as e:
            logger.error(f"íŒ¨í„´ ìµœì í™” ì‹¤íŒ¨: {e}")
            return [] 