from motor.motor_asyncio import AsyncIOMotorDatabase
from .llm_service import LLMService
from .db_enhancement_service import DBEnhancementService
from .conversation_algorithm import ConversationAlgorithm
from .formatting_service import FormattingService
from .model_manager import get_model_manager, ModelType
from .input_filter import InputFilter, InputType as InputFilterType
from ..config import settings, enable_module, disable_module, get_module_status
import logging
import time
from datetime import datetime
from typing import Dict, List, Optional, Any
import asyncio

class ChatService:
    """
    ì±„íŒ… ì„œë¹„ìŠ¤ - ê³ ê° ì‘ëŒ€ ì•Œê³ ë¦¬ì¦˜ ê¸°ë°˜
    
    ì—…ë¬´ë³„ ë©”ì„œë“œ ë¶„ë¦¬:
    - get_conversation_response: ëŒ€í™” ì •ë³´ ë°›ê¸° (ì¼ìƒ ëŒ€í™”)
    - search_and_enhance_answer: ì‘ë‹µ ì°¾ê¸° (ì „ë¬¸ ìƒë‹´)
    - format_and_send_response: ì‘ë‹µ ì •ë³´ ë³´ë‚´ê¸° (í¬ë§·íŒ…)
    """
    
    def __init__(self, db: AsyncIOMotorDatabase):
        """
        ì±„íŒ… ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        
        Args:
            db: MongoDB ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¸ìŠ¤í„´ìŠ¤
        """
        self.db = db
        
        # DB ì—°ê³„ ì„œë¹„ìŠ¤ (LLMê³¼ ë¶„ë¦¬)
        self.db_enhancement_service = DBEnhancementService(db)
        
        # ê³ ê° ì‘ëŒ€ ì•Œê³ ë¦¬ì¦˜
        self.conversation_algorithm = ConversationAlgorithm()
        
        # ì…ë ¥ í•„í„° (ì„¤ì •ì— ë”°ë¼ ì„ íƒ)
        if settings.ENABLE_CONTEXT_AWARE_CLASSIFICATION:
            from .context_aware_classifier import ContextAwareClassifier, InputType
            self.input_filter = ContextAwareClassifier(db)
            # LLM ì„œë¹„ìŠ¤ ì£¼ì… (ë‚˜ì¤‘ì— ì£¼ì…)
            self.input_filter.inject_llm_service(None)  # ì´ˆê¸°í™” ì‹œì—ëŠ” Noneìœ¼ë¡œ ì„¤ì •
            self.InputType = InputType  # context_aware_classifierì˜ InputType ì‚¬ìš©
            logging.info("âœ… ë¬¸ë§¥ ì¸ì‹ ë¶„ë¥˜ê¸° í™œì„±í™”")
        else:
            # DB ì—°ë™ëœ InputFilter ì‚¬ìš©
            self.input_filter = InputFilter(db)
            self.InputType = InputFilterType  # input_filterì˜ InputType ì‚¬ìš©
            logging.info("âœ… DB ì—°ë™ í‚¤ì›Œë“œ ë¶„ë¥˜ê¸° ì‚¬ìš©")
        
        # ìë™í™” ì„œë¹„ìŠ¤ ì¶”ê°€
        from .automation_service import AutomationService
        self.automation_service = AutomationService(db)
        
        logging.info("ChatService ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _initialize_modules(self):
        """ì„¤ì •ì— ë”°ë¼ ëª¨ë“ˆì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        # DB ì—°ê³„ ì„œë¹„ìŠ¤ (ì„¤ì •ì— ë”°ë¼ í™œì„±í™”/ë¹„í™œì„±í™”)
        if settings.ENABLE_MONGODB_SEARCH:
            self.db_enhancement_service = DBEnhancementService(self.db)
            logging.info("âœ… MongoDB ê²€ìƒ‰ ëª¨ë“ˆ í™œì„±í™”")
        else:
            self.db_enhancement_service = None
            logging.info("âŒ MongoDB ê²€ìƒ‰ ëª¨ë“ˆ ë¹„í™œì„±í™”")
        
        # ê³ ê° ì‘ëŒ€ ì•Œê³ ë¦¬ì¦˜ (ì„¤ì •ì— ë”°ë¼ í™œì„±í™”/ë¹„í™œì„±í™”)
        if settings.ENABLE_CONVERSATION_ANALYSIS:
            self.conversation_algorithm = ConversationAlgorithm()
            logging.info("âœ… ê³ ê° ì§ˆë¬¸ ë¶„ì„ ëª¨ë“ˆ í™œì„±í™”")
        else:
            self.conversation_algorithm = None
            logging.info("âŒ ê³ ê° ì§ˆë¬¸ ë¶„ì„ ëª¨ë“ˆ ë¹„í™œì„±í™”")
        
        # ì…ë ¥ í•„í„° (ì„¤ì •ì— ë”°ë¼ í™œì„±í™”/ë¹„í™œì„±í™”)
        if settings.ENABLE_INPUT_FILTERING:
            self.input_filter = get_input_filter()
            logging.info("âœ… ì…ë ¥ í•„í„°ë§ ëª¨ë“ˆ í™œì„±í™”")
        else:
            self.input_filter = None
            logging.info("âŒ ì…ë ¥ í•„í„°ë§ ëª¨ë“ˆ ë¹„í™œì„±í™”")
    
    def _log_module_status(self):
        """ëª¨ë“ˆ ìƒíƒœë¥¼ ë¡œê¹…í•©ë‹ˆë‹¤."""
        status = get_module_status()
        logging.info("=== ëª¨ë“ˆ í™œì„±í™” ìƒíƒœ ===")
        for module, enabled in status.items():
            status_str = "âœ… í™œì„±í™”" if enabled else "âŒ ë¹„í™œì„±í™”"
            logging.info(f"{module}: {status_str}")
        logging.info("=======================")

    # ===== ì—…ë¬´ë³„ ë©”ì„œë“œ ë¶„ë¦¬ =====

    async def get_conversation_response(self, message: str) -> str:
        """
        ëŒ€í™” ì •ë³´ ë°›ê¸° - ì¼ìƒ ëŒ€í™” ì²˜ë¦¬ (LLM ì „ìš©)
        
        Args:
            message: ì‚¬ìš©ì ë©”ì‹œì§€
            
        Returns:
            ì¼ìƒ ëŒ€í™” ì‘ë‹µ
        """
        try:
            logging.info(f"ì¼ìƒ ëŒ€í™” ì²˜ë¦¬ ì‹œì‘: {message[:50]}...")
            
            # LLM ì„œë¹„ìŠ¤ë¥¼ í†µí•œ ì¼ìƒ ëŒ€í™” ì²˜ë¦¬
            response = await self.llm_service.get_conversation_response(message)
            
            self.response_stats['casual_conversations'] += 1
            self.response_stats['llm_responses'] += 1
            
            logging.info(f"ì¼ìƒ ëŒ€í™” ì²˜ë¦¬ ì™„ë£Œ: {response[:50]}...")
            return response
            
        except Exception as e:
            logging.error(f"ì¼ìƒ ëŒ€í™” ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
            self.response_stats['errors'] += 1
            
            # ê¸°ë³¸ ì‘ë‹µìœ¼ë¡œ ì•ˆì •ì„± í™•ë³´
            return self.conversation_algorithm._generate_casual_response(message)

    async def search_and_enhance_answer(self, message: str) -> str:
        """
        ì‘ë‹µ ì°¾ê¸° - ì „ë¬¸ ìƒë‹´ ì²˜ë¦¬ (LLM ì „ìš©)
        
        Args:
            message: ì‚¬ìš©ì ë©”ì‹œì§€
            
        Returns:
            ì „ë¬¸ ìƒë‹´ ì‘ë‹µ
        """
        try:
            logging.info(f"ì „ë¬¸ ìƒë‹´ ì²˜ë¦¬ ì‹œì‘: {message[:50]}...")
            
            # LLM ì„œë¹„ìŠ¤ë¥¼ í†µí•œ ì „ë¬¸ ìƒë‹´ ì²˜ë¦¬
            response = await self.llm_service.search_and_enhance_answer(message)
            
            self.response_stats['professional_conversations'] += 1
            self.response_stats['llm_responses'] += 1
            
            logging.info(f"ì „ë¬¸ ìƒë‹´ ì²˜ë¦¬ ì™„ë£Œ: {response[:50]}...")
            return response
            
        except Exception as e:
            logging.error(f"ì „ë¬¸ ìƒë‹´ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
            self.response_stats['errors'] += 1
            
            # ê¸°ë³¸ ì‘ë‹µìœ¼ë¡œ ì•ˆì •ì„± í™•ë³´
            return self.conversation_algorithm.generate_no_answer_response(message)

    async def format_and_send_response(self, response: str) -> str:
        """
        ì‘ë‹µ ì •ë³´ ë³´ë‚´ê¸° - ì‘ë‹µ í¬ë§·íŒ… (LLM ì „ìš©)
        
        Args:
            response: ì›ë³¸ ì‘ë‹µ
            
        Returns:
            í¬ë§·íŒ…ëœ ì‘ë‹µ
        """
        try:
            logging.info("ì‘ë‹µ í¬ë§·íŒ… ì‹œì‘")
            
            # LLM ì„œë¹„ìŠ¤ë¥¼ í†µí•œ í¬ë§·íŒ…
            formatted_response = await self.llm_service.format_and_send_response(response)
            
            logging.info(f"ì‘ë‹µ í¬ë§·íŒ… ì™„ë£Œ: {formatted_response[:50]}...")
            return formatted_response
            
        except Exception as e:
            logging.error(f"ì‘ë‹µ í¬ë§·íŒ… ì˜¤ë¥˜: {str(e)}")
            self.response_stats['errors'] += 1
            
            # ê¸°ë³¸ í¬ë§·íŒ…ìœ¼ë¡œ ì•ˆì •ì„± í™•ë³´
            return FormattingService.format_response(response)

    # ===== í†µí•© ë©”ì„œë“œ =====

    async def process_message(self, message: str, conversation_id: str = None) -> str:
        """
        ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ì²˜ë¦¬í•˜ê³  ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.
        
        Args:
            message: ì‚¬ìš©ì ë©”ì‹œì§€
            conversation_id: ëŒ€í™” ID
            
        Returns:
            str: AI ì‘ë‹µ
        """
        start_time = time.time()
        
        try:
            logging.info(f"ë©”ì‹œì§€ ì²˜ë¦¬ ì‹œì‘: {message[:20]}...")
            
            # 1. ì…ë ¥ ë¶„ë¥˜
            input_type, details = await self.input_filter.classify_input(message)
            logging.info(f"ì…ë ¥ ë¶„ë¥˜: {input_type.value} - {details.get('reason', '')}")
            
            # 2. ë¶„ë¥˜ì— ë”°ë¥¸ ì‘ë‹µ ìƒì„±
            if input_type in [self.InputType.PROFANITY, self.InputType.NON_COUNSELING]:
                # í…œí”Œë¦¿ ì‘ë‹µ ì‚¬ìš©
                response = self.input_filter.get_response_template(input_type)
                logging.info(f"í…œí”Œë¦¿ ì‘ë‹µ ì‚¬ìš©: {response}")
            else:
                # LLM ë˜ëŠ” DB ê¸°ë°˜ ì‘ë‹µ
                if input_type in [self.InputType.TECHNICAL, self.InputType.UNKNOWN]:
                    # ì „ë¬¸ ìƒë‹´ ì²˜ë¦¬ (unknownë„ í¬í•¨)
                    logging.info(f"ğŸ” {input_type.value.upper()} ë¶„ë¥˜ ê°ì§€ - ì „ë¬¸ ìƒë‹´ ì²˜ë¦¬ ì‹œì‘")
                    response = await self._handle_technical_conversation(message)
                    logging.info(f"âœ… ì „ë¬¸ ìƒë‹´ ì²˜ë¦¬ ì™„ë£Œ: {response[:100]}...")
                else:
                    # ì¼ìƒ ëŒ€í™” ì²˜ë¦¬ (casualë§Œ)
                    logging.info("ğŸ” CASUAL ë¶„ë¥˜ ê°ì§€ - ì¼ìƒ ëŒ€í™” ì²˜ë¦¬ ì‹œì‘")
                    response = await self._handle_casual_conversation(message)
                    logging.info(f"âœ… ì¼ìƒ ëŒ€í™” ì²˜ë¦¬ ì™„ë£Œ: {response[:100]}...")
            
            # 3. ìë™í™” ì²˜ë¦¬ (ëŒ€í™” ì €ì¥ ë° knowledge_base ì—…ë°ì´íŠ¸)
            automation_result = await self.automation_service.process_conversation_automation(
                message, response, input_type.value
            )
            
            # 4. ì‘ë‹µ í¬ë§·íŒ…
            formatted_response = await self._format_response(response, input_type)
            
            # 5. ì²˜ë¦¬ ì‹œê°„ ê¸°ë¡
            end_time = time.time()
            processing_time = (end_time - start_time) * 1000
            logging.info(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] ìƒë‹´ì‚¬ ì‘ë‹µ ì™„ë£Œ")
            logging.info(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ms")
            logging.info(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] ì‘ë‹µ ë‚´ìš©:\n{formatted_response}")
            
            return formatted_response
            
        except Exception as e:
            logging.error(f"ë©”ì‹œì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ì¼ì‹œì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."

    async def _handle_casual_conversation(self, message: str) -> str:
        """ì¼ìƒ ëŒ€í™” ì²˜ë¦¬"""
        try:
            # ê¸°ì¡´ LLM ì„œë¹„ìŠ¤ ì‚¬ìš© (ìƒˆë¡œ ìƒì„±í•˜ì§€ ì•ŠìŒ)
            from ..dependencies import get_llm_service
            llm_service = await get_llm_service()
            response = await llm_service.get_conversation_response(message)
            return response
        except Exception as e:
            logging.error(f"ì¼ìƒ ëŒ€í™” ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
            return "ì•ˆë…•í•˜ì„¸ìš”! ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”?"

    async def _handle_technical_conversation(self, message: str) -> str:
        """ì „ë¬¸ ìƒë‹´ ì²˜ë¦¬"""
        try:
            logging.info("ğŸ” ì „ë¬¸ ìƒë‹´ ì²˜ë¦¬ ì‹œì‘")
            
            # ê¸°ì¡´ LLM ì„œë¹„ìŠ¤ ì‚¬ìš© (ìƒˆë¡œ ìƒì„±í•˜ì§€ ì•ŠìŒ)
            from ..dependencies import get_llm_service
            llm_service = await get_llm_service()
            logging.info("âœ… LLM ì„œë¹„ìŠ¤ ê°€ì ¸ì˜¤ê¸° ì™„ë£Œ")
            
            # DB ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
            if hasattr(llm_service, 'search_service') and llm_service.search_service:
                logging.info("âœ… DB ê²€ìƒ‰ ì„œë¹„ìŠ¤ê°€ ì£¼ì…ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            else:
                logging.warning("âŒ DB ê²€ìƒ‰ ì„œë¹„ìŠ¤ê°€ ì£¼ì…ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. DB ëª¨ë“œë¡œ ì¬ì„¤ì •í•©ë‹ˆë‹¤.")
                # DB ì„œë¹„ìŠ¤ ì¬ì£¼ì…
                from ..dependencies import get_search_service
                search_service = await get_search_service()
                llm_service.inject_db_service(search_service)
                llm_service.set_db_mode(True)
                logging.info("âœ… DB ê²€ìƒ‰ ì„œë¹„ìŠ¤ ì¬ì£¼ì… ì™„ë£Œ")
            
            # DB ê²€ìƒ‰ + LLM ê°•í™”
            logging.info("ğŸ” search_and_enhance_answer í˜¸ì¶œ ì‹œì‘")
            response = await llm_service.search_and_enhance_answer(message)
            logging.info(f"âœ… search_and_enhance_answer ì™„ë£Œ: {response[:100]}...")
            return response
        except Exception as e:
            logging.error(f"ì „ë¬¸ ìƒë‹´ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ì „ë¬¸ ìƒë‹´ì‚¬ì—ê²Œ ë¬¸ì˜í•´ì£¼ì„¸ìš”."

    async def _format_response(self, response: str, input_type=None) -> str:
        """ì‘ë‹µ í¬ë§·íŒ…"""
        try:
            if not response:
                return "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            # ê¸°ë³¸ í¬ë§·íŒ…
            formatted = response.strip()
            
            # technical/unknown ë¶„ë¥˜ì¼ ë•ŒëŠ” ê¸¸ì´ ì œí•œ í•´ì œ (DB ë‹µë³€ ë³´ì¡´)
            if input_type in [self.InputType.TECHNICAL, self.InputType.UNKNOWN]:
                logging.info(f"âœ… {input_type.value.upper()} ë¶„ë¥˜ - ì‘ë‹µ ê¸¸ì´ ì œí•œ í•´ì œ")
                return formatted
            
            # ë„ˆë¬´ ê¸´ ì‘ë‹µ ìë¥´ê¸° (ì¼ìƒ ëŒ€í™”ì—ë§Œ ì ìš©)
            if len(formatted) > 1000:
                formatted = formatted[:1000] + "..."
            
            return formatted
        except Exception as e:
            logging.error(f"ì‘ë‹µ í¬ë§·íŒ… ì˜¤ë¥˜: {str(e)}")
            return response

    # ===== ê¸°ì¡´ ë©”ì„œë“œë“¤ (í˜¸í™˜ì„± ìœ ì§€) =====

    def switch_model(self, model_type: str) -> bool:
        """ëª¨ë¸ ì „í™˜ (í˜¸í™˜ì„± ìœ ì§€)"""
        try:
            success = self.model_manager.switch_model(model_type)
            if success and self.llm_service:
                self.llm_service.model_type = model_type
            return success
        except Exception as e:
            logging.error(f"ëª¨ë¸ ì „í™˜ ì˜¤ë¥˜: {str(e)}")
            return False

    def get_current_model(self):
        """í˜„ì¬ ëª¨ë¸ ë°˜í™˜ (í˜¸í™˜ì„± ìœ ì§€)"""
        return self.model_manager.get_current_model()

    def get_current_model_config(self):
        """í˜„ì¬ ëª¨ë¸ ì„¤ì • ë°˜í™˜ (í˜¸í™˜ì„± ìœ ì§€)"""
        return self.model_manager.get_current_model_config()

    def get_response_stats(self) -> Dict:
        """ì‘ë‹µ í†µê³„ ë°˜í™˜"""
        stats = self.response_stats.copy()
        
        if stats['total_requests'] > 0:
            stats['avg_processing_time'] = stats['total_processing_time'] / stats['total_requests']
            if stats['db_responses'] > 0:
                stats['avg_db_processing_time'] = stats['total_processing_time'] / stats['db_responses']
            if stats['llm_responses'] > 0:
                stats['avg_llm_processing_time'] = stats['total_processing_time'] / stats['llm_responses']
            else:
                stats['avg_llm_processing_time'] = 0
            
            # ì¤‘ê°„ê°’ ê³„ì‚°
            if stats['processing_times']:
                sorted_times = sorted(stats['processing_times'])
                stats['median_processing_time'] = sorted_times[len(sorted_times) // 2]
        else:
            stats['avg_processing_time'] = 0
            stats['avg_db_processing_time'] = 0
            stats['avg_llm_processing_time'] = 0
            stats['median_processing_time'] = 0
        
        return stats

    def log_response_stats(self):
        """ì‘ë‹µ í†µê³„ë¥¼ ë¡œê·¸ë¡œ ì¶œë ¥"""
        stats = self.get_response_stats()
        
        logging.info("=== ChatService ì‘ë‹µ í†µê³„ ===")
        logging.info(f"ì´ ìš”ì²­ ìˆ˜: {stats['total_requests']}")
        logging.info(f"ì¼ìƒ ëŒ€í™”: {stats['casual_conversations']}")
        logging.info(f"ì „ë¬¸ ìƒë‹´: {stats['professional_conversations']}")
        logging.info(f"DB ì‘ë‹µ: {stats['db_responses']}")
        logging.info(f"LLM ì‘ë‹µ: {stats['llm_responses']}")
        logging.info(f"ì˜¤ë¥˜: {stats['errors']}")
        logging.info(f"í‰ê·  ì²˜ë¦¬ ì‹œê°„: {stats['avg_processing_time']:.2f}ms")
        logging.info(f"ìµœì†Œ ì²˜ë¦¬ ì‹œê°„: {stats['min_processing_time']:.2f}ms")
        logging.info(f"ìµœëŒ€ ì²˜ë¦¬ ì‹œê°„: {stats['max_processing_time']:.2f}ms")
        logging.info(f"ì¤‘ê°„ê°’ ì²˜ë¦¬ ì‹œê°„: {stats['median_processing_time']:.2f}ms")
        logging.info("=============================")

    def set_db_priority_mode(self, enabled: bool):
        """DB ìš°ì„  ëª¨ë“œ ì„¤ì • (í˜¸í™˜ì„± ìœ ì§€)"""
        if self.llm_service:
            self.llm_service.set_db_priority_mode(enabled)

    def get_model_status(self) -> Dict[str, Any]:
        """ëª¨ë¸ ìƒíƒœ ë°˜í™˜ (í˜¸í™˜ì„± ìœ ì§€)"""
        return {
            "current_model": self.model_manager.current_model,
            "available_models": self.model_manager.get_available_models(),
            "loaded_models": self.model_manager.get_loaded_models()
        }